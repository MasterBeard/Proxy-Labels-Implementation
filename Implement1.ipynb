{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ8+LqktD33EFisGaFbkK6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasterBeard/Proxy-Labels-Implementation/blob/main/Implement1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUz5xqM49LWU",
        "outputId": "360f6c99-bdf4-4835-fe27-c72ddd660922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 4x4 matrices shape: (44871, 4, 5, 1)\n",
            "Train 1st matrices shape: (44871, 32)\n",
            "Train labels1 shape: (44871,)\n",
            "Train origin shape: (44871,)\n",
            "Validation 4x4 matrices shape: (23353, 4, 5, 1)\n",
            "Validation 1st matrices shape: (23353, 32)\n",
            "Validation labels1 shape: (23353,)\n",
            "Validation origin shape: (23353,)\n",
            "Test 4x4 matrices shape: (22604, 4, 5, 1)\n",
            "Test 1st matrices shape: (22604, 32)\n",
            "Test labels1 shape: (22604,)\n",
            "Test origin shape: (22604,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy.polynomial.polynomial as poly\n",
        "\n",
        "# Define stock and index tickers\n",
        "stock_tickers = [\n",
        "    'OKE', 'ENPH', 'UHS', 'DLTR', 'AMZN', 'EFX', 'RSG', 'OXY', 'REGN', 'DECK',\n",
        "    '^GSPC',  # S&P 500\n",
        "    '^IXIC',  # NASDAQ Composite\n",
        "    '^HSI',   # Hang Seng Index\n",
        "    '^DJI',   # Dow Jones Industrial Average\n",
        "    '^FCHI',  # CAC 40\n",
        "    '^GDAXI', # DAX\n",
        "    '^N225',  # Nikkei 225\n",
        "    '^KS11',  # KOSPI\n",
        "    '^STOXX50E'  # EURO STOXX 50\n",
        "]\n",
        "\n",
        "# Define date ranges\n",
        "date_ranges = {\n",
        "    'train': (\"2005-01-01\", \"2015-01-01\"),\n",
        "    'val': (\"2015-01-02\", \"2019-12-31\"),\n",
        "    'test': (\"2020-01-01\", \"2024-10-31\")\n",
        "}\n",
        "\n",
        "# Store matrices, labels, and origin for each split\n",
        "data_splits = {\n",
        "    split: {\n",
        "        'matrices_4x4': [],\n",
        "        'matrices_1st': [],\n",
        "        'labels4': [],\n",
        "        'labels1': [],\n",
        "        'labels2': [],\n",
        "        'labels3': [],\n",
        "        'origin': []  # Record the origin (stock or index) of each matrices_4x4\n",
        "    }\n",
        "    for split in date_ranges\n",
        "}\n",
        "\n",
        "# Window length\n",
        "window_size = 25\n",
        "degree = 2\n",
        "\n",
        "# Fetch and process data for each time period\n",
        "for split, (start_date, end_date) in date_ranges.items():\n",
        "    # Download all data (stocks and indices) for the specified date range\n",
        "    all_data = {ticker: yf.download(ticker, start=start_date, end=end_date, auto_adjust=False) for ticker in stock_tickers}\n",
        "\n",
        "    # Create first and second derivative matrices for each stock and index\n",
        "    for ticker, data in all_data.items():\n",
        "        # Extract required 'Open' and 'Close' data\n",
        "        open_values = data['Open'].dropna().values\n",
        "        close_values = data['Close'].dropna().values\n",
        "\n",
        "        # Build window matrices\n",
        "        for start in range(len(data) - window_size + 1):\n",
        "            # Extract each row of data\n",
        "            open_row = open_values[start:start + window_size]\n",
        "            close_row = close_values[start:start + window_size]\n",
        "\n",
        "            # Normalize using the 16th last value of close_row\n",
        "            normalization_factor = close_row[-16]\n",
        "            open_row = open_row / normalization_factor\n",
        "            close_row = close_row / normalization_factor\n",
        "\n",
        "            # Create an alternating combined array\n",
        "            combined = np.array([open_row[i // 2] if i % 2 == 0 else close_row[i // 2] for i in range(window_size * 2)])\n",
        "            matrix_4x4 = combined[:-30].reshape(4, 5, 1)\n",
        "            matrix_4x5 = combined[-32:].reshape(-1)\n",
        "            result = [combined[-32], combined[-31], combined[-1]]\n",
        "            data_splits[split]['matrices_4x4'].append(matrix_4x4)\n",
        "            data_splits[split]['matrices_1st'].append(matrix_4x5)\n",
        "            data_splits[split]['origin'].append(ticker)  # Record origin (stock or index)\n",
        "\n",
        "            # Perform higher-order fitting on combined data\n",
        "            x = np.arange(len(combined[-4:-1]))\n",
        "            coeffs = poly.polyfit(x, result, deg=degree)\n",
        "\n",
        "            # Calculate first derivative\n",
        "            first_derivative_coeffs = poly.polyder(coeffs)\n",
        "            first_derivatives = poly.polyval(x, first_derivative_coeffs)\n",
        "\n",
        "            # Calculate second derivative\n",
        "            second_derivative_coeffs = poly.polyder(first_derivative_coeffs)\n",
        "            second_derivatives = poly.polyval(x, second_derivative_coeffs)\n",
        "\n",
        "            # Generate labels\n",
        "            label1 = 1 if first_derivatives[0][-1] > 0 else 0\n",
        "            label2 = 1 if second_derivatives[0][-1] > 0 else 0\n",
        "            label3 = 1 if close_row[-1] > close_row[-16] else 0\n",
        "            label4 = [first_derivatives[-1], second_derivatives[-1]]\n",
        "\n",
        "            data_splits[split]['labels1'].append(label1)\n",
        "            data_splits[split]['labels2'].append(label2)\n",
        "            data_splits[split]['labels3'].append(label3)\n",
        "            data_splits[split]['labels4'].append(label4)\n",
        "\n",
        "# Convert matrices in each split to NumPy arrays\n",
        "def convert_to_numpy(split):\n",
        "    return (\n",
        "        np.array(data_splits[split]['matrices_4x4']),\n",
        "        np.array(data_splits[split]['matrices_1st']),\n",
        "        np.array(data_splits[split]['labels1']),\n",
        "        np.array(data_splits[split]['labels2']),\n",
        "        np.array(data_splits[split]['labels3']),\n",
        "        np.array(data_splits[split]['labels4']),\n",
        "        np.array(data_splits[split]['origin'])\n",
        "    )\n",
        "\n",
        "train_data = convert_to_numpy('train')\n",
        "val_data = convert_to_numpy('val')\n",
        "test_data = convert_to_numpy('test')\n",
        "\n",
        "# Output shapes of each split to check results\n",
        "print(f\"Train 4x4 matrices shape: {train_data[0].shape}\")\n",
        "print(f\"Train 1st matrices shape: {train_data[1].shape}\")\n",
        "print(f\"Train labels1 shape: {train_data[2].shape}\")\n",
        "print(f\"Train origin shape: {train_data[6].shape}\")\n",
        "\n",
        "print(f\"Validation 4x4 matrices shape: {val_data[0].shape}\")\n",
        "print(f\"Validation 1st matrices shape: {val_data[1].shape}\")\n",
        "print(f\"Validation labels1 shape: {val_data[2].shape}\")\n",
        "print(f\"Validation origin shape: {val_data[6].shape}\")\n",
        "\n",
        "print(f\"Test 4x4 matrices shape: {test_data[0].shape}\")\n",
        "print(f\"Test 1st matrices shape: {test_data[1].shape}\")\n",
        "print(f\"Test labels1 shape: {test_data[2].shape}\")\n",
        "print(f\"Test origin shape: {test_data[6].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_labels(labels1, labels2, labels3):\n",
        "    \"\"\"\n",
        "    Generate new grouped labels based on the permutations of the three input labels.\n",
        "    Each combination is uniquely encoded: e.g., (0, 1, 1) → 3.\n",
        "    \"\"\"\n",
        "    # Combine labels into a unique code using weighted sum: labels1 * 4 + labels2 * 2 + labels3 * 1\n",
        "    combined_labels = labels1 * 4 + labels2 * 2 + labels3 * 1\n",
        "    return combined_labels\n",
        "\n",
        "# Regenerate labels for train, validation, and test sets\n",
        "train_labels = combine_labels(train_data[2], train_data[3], train_data[4])\n",
        "val_labels = combine_labels(val_data[2], val_data[3], val_data[4])\n",
        "test_labels = combine_labels(test_data[2], test_data[3], test_data[4])\n",
        "\n",
        "# Print label distributions to check the combinations\n",
        "print(\"Train Labels Distribution:\", np.unique(train_labels, return_counts=True))\n",
        "print(\"Validation Labels Distribution:\", np.unique(val_labels, return_counts=True))\n",
        "print(\"Test Labels Distribution:\", np.unique(test_labels, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSJ_Cs0U-CNI",
        "outputId": "bf3064cf-71f8-4a5c-9d51-1b3e8a81e4af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Labels Distribution: (array([0, 1, 2, 5, 6, 7]), array([17051,   559,   859,  1096,   438, 24868]))\n",
            "Validation Labels Distribution: (array([0, 1, 2, 5, 6, 7]), array([ 9210,   263,   480,   560,   264, 12576]))\n",
            "Test Labels Distribution: (array([0, 1, 2, 5, 6, 7]), array([ 9010,   241,   446,   492,   225, 12190]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Build MLP model\n",
        "model1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(4, 5, 1)),  # Flatten (4, 5, 1) into (20,)\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),  # First Dense layer\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),  # Second Dense layer\n",
        "    tf.keras.layers.Dense(units=8, activation='softmax')  # 8-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model1.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',  # Suitable for integer labels\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history1 = model1.fit(\n",
        "    train_data[0], train_labels,  # Training data (inputs, labels)\n",
        "    epochs=300,\n",
        "    batch_size=128,\n",
        "    validation_data=(val_data[0], val_labels),  # Validation data\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save model1\n",
        "model1.save('modeltaskmlp1.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzOf4Nnt9wDj",
        "outputId": "ef42cf87-03a3-4c13-eae9-e5c1032a783f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5260 - loss: 1.0732 - val_accuracy: 0.5385 - val_loss: 0.9834\n",
            "Epoch 2/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5525 - loss: 0.9710 - val_accuracy: 0.5385 - val_loss: 0.9756\n",
            "Epoch 3/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5541 - loss: 0.9626 - val_accuracy: 0.5385 - val_loss: 0.9760\n",
            "Epoch 4/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5547 - loss: 0.9657 - val_accuracy: 0.5385 - val_loss: 0.9724\n",
            "Epoch 5/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5565 - loss: 0.9605 - val_accuracy: 0.5385 - val_loss: 0.9769\n",
            "Epoch 6/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5536 - loss: 0.9675 - val_accuracy: 0.5385 - val_loss: 0.9734\n",
            "Epoch 7/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5588 - loss: 0.9604 - val_accuracy: 0.5385 - val_loss: 0.9741\n",
            "Epoch 8/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5554 - loss: 0.9565 - val_accuracy: 0.5385 - val_loss: 0.9717\n",
            "Epoch 9/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5553 - loss: 0.9647 - val_accuracy: 0.5385 - val_loss: 0.9750\n",
            "Epoch 10/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5536 - loss: 0.9682 - val_accuracy: 0.5385 - val_loss: 0.9727\n",
            "Epoch 11/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5525 - loss: 0.9639 - val_accuracy: 0.5385 - val_loss: 0.9722\n",
            "Epoch 12/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5528 - loss: 0.9642 - val_accuracy: 0.5385 - val_loss: 0.9755\n",
            "Epoch 13/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5576 - loss: 0.9609 - val_accuracy: 0.5385 - val_loss: 0.9719\n",
            "Epoch 14/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5542 - loss: 0.9644 - val_accuracy: 0.5385 - val_loss: 0.9759\n",
            "Epoch 15/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5555 - loss: 0.9553 - val_accuracy: 0.5385 - val_loss: 0.9692\n",
            "Epoch 16/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5540 - loss: 0.9589 - val_accuracy: 0.5385 - val_loss: 0.9690\n",
            "Epoch 17/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5503 - loss: 0.9647 - val_accuracy: 0.5386 - val_loss: 0.9678\n",
            "Epoch 18/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5529 - loss: 0.9699 - val_accuracy: 0.5385 - val_loss: 0.9692\n",
            "Epoch 19/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5542 - loss: 0.9553 - val_accuracy: 0.5385 - val_loss: 0.9662\n",
            "Epoch 20/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5513 - loss: 0.9595 - val_accuracy: 0.5385 - val_loss: 0.9669\n",
            "Epoch 21/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5558 - loss: 0.9528 - val_accuracy: 0.5386 - val_loss: 0.9650\n",
            "Epoch 22/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5566 - loss: 0.9540 - val_accuracy: 0.5384 - val_loss: 0.9638\n",
            "Epoch 23/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5574 - loss: 0.9496 - val_accuracy: 0.5386 - val_loss: 0.9672\n",
            "Epoch 24/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5526 - loss: 0.9548 - val_accuracy: 0.5386 - val_loss: 0.9621\n",
            "Epoch 25/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5611 - loss: 0.9514 - val_accuracy: 0.5384 - val_loss: 0.9612\n",
            "Epoch 26/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5594 - loss: 0.9479 - val_accuracy: 0.5387 - val_loss: 0.9652\n",
            "Epoch 27/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5545 - loss: 0.9524 - val_accuracy: 0.5382 - val_loss: 0.9671\n",
            "Epoch 28/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5564 - loss: 0.9476 - val_accuracy: 0.5387 - val_loss: 0.9579\n",
            "Epoch 29/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5535 - loss: 0.9483 - val_accuracy: 0.5387 - val_loss: 0.9566\n",
            "Epoch 30/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5566 - loss: 0.9429 - val_accuracy: 0.5386 - val_loss: 0.9542\n",
            "Epoch 31/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5525 - loss: 0.9406 - val_accuracy: 0.5384 - val_loss: 0.9630\n",
            "Epoch 32/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5532 - loss: 0.9405 - val_accuracy: 0.5384 - val_loss: 0.9552\n",
            "Epoch 33/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5520 - loss: 0.9412 - val_accuracy: 0.5382 - val_loss: 0.9661\n",
            "Epoch 34/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5542 - loss: 0.9394 - val_accuracy: 0.5386 - val_loss: 0.9500\n",
            "Epoch 35/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5575 - loss: 0.9313 - val_accuracy: 0.5385 - val_loss: 0.9632\n",
            "Epoch 36/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5535 - loss: 0.9437 - val_accuracy: 0.5386 - val_loss: 0.9464\n",
            "Epoch 37/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5526 - loss: 0.9376 - val_accuracy: 0.5384 - val_loss: 0.9491\n",
            "Epoch 38/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5581 - loss: 0.9298 - val_accuracy: 0.5385 - val_loss: 0.9438\n",
            "Epoch 39/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5565 - loss: 0.9339 - val_accuracy: 0.5386 - val_loss: 0.9440\n",
            "Epoch 40/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5544 - loss: 0.9374 - val_accuracy: 0.5386 - val_loss: 0.9505\n",
            "Epoch 41/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5540 - loss: 0.9368 - val_accuracy: 0.5384 - val_loss: 0.9502\n",
            "Epoch 42/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5503 - loss: 0.9298 - val_accuracy: 0.5394 - val_loss: 0.9399\n",
            "Epoch 43/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5588 - loss: 0.9309 - val_accuracy: 0.5386 - val_loss: 0.9428\n",
            "Epoch 44/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5588 - loss: 0.9340 - val_accuracy: 0.5389 - val_loss: 0.9419\n",
            "Epoch 45/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5561 - loss: 0.9224 - val_accuracy: 0.5385 - val_loss: 0.9379\n",
            "Epoch 46/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5593 - loss: 0.9251 - val_accuracy: 0.5387 - val_loss: 0.9432\n",
            "Epoch 47/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5581 - loss: 0.9248 - val_accuracy: 0.5386 - val_loss: 0.9389\n",
            "Epoch 48/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5594 - loss: 0.9230 - val_accuracy: 0.5389 - val_loss: 0.9390\n",
            "Epoch 49/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5529 - loss: 0.9284 - val_accuracy: 0.5387 - val_loss: 0.9381\n",
            "Epoch 50/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5521 - loss: 0.9272 - val_accuracy: 0.5396 - val_loss: 0.9356\n",
            "Epoch 51/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5542 - loss: 0.9317 - val_accuracy: 0.5394 - val_loss: 0.9353\n",
            "Epoch 52/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5587 - loss: 0.9281 - val_accuracy: 0.5398 - val_loss: 0.9531\n",
            "Epoch 53/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5584 - loss: 0.9265 - val_accuracy: 0.5388 - val_loss: 0.9401\n",
            "Epoch 54/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5519 - loss: 0.9322 - val_accuracy: 0.5388 - val_loss: 0.9436\n",
            "Epoch 55/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5599 - loss: 0.9238 - val_accuracy: 0.5401 - val_loss: 0.9354\n",
            "Epoch 56/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5570 - loss: 0.9253 - val_accuracy: 0.5397 - val_loss: 0.9392\n",
            "Epoch 57/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5575 - loss: 0.9250 - val_accuracy: 0.5397 - val_loss: 0.9380\n",
            "Epoch 58/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5572 - loss: 0.9161 - val_accuracy: 0.5409 - val_loss: 0.9461\n",
            "Epoch 59/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5531 - loss: 0.9326 - val_accuracy: 0.5398 - val_loss: 0.9448\n",
            "Epoch 60/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5569 - loss: 0.9249 - val_accuracy: 0.5395 - val_loss: 0.9381\n",
            "Epoch 61/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5562 - loss: 0.9266 - val_accuracy: 0.5398 - val_loss: 0.9348\n",
            "Epoch 62/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5553 - loss: 0.9274 - val_accuracy: 0.5392 - val_loss: 0.9316\n",
            "Epoch 63/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5552 - loss: 0.9203 - val_accuracy: 0.5392 - val_loss: 0.9399\n",
            "Epoch 64/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5593 - loss: 0.9200 - val_accuracy: 0.5393 - val_loss: 0.9392\n",
            "Epoch 65/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5555 - loss: 0.9206 - val_accuracy: 0.5394 - val_loss: 0.9387\n",
            "Epoch 66/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5582 - loss: 0.9173 - val_accuracy: 0.5392 - val_loss: 0.9379\n",
            "Epoch 67/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5545 - loss: 0.9227 - val_accuracy: 0.5416 - val_loss: 0.9355\n",
            "Epoch 68/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5589 - loss: 0.9162 - val_accuracy: 0.5392 - val_loss: 0.9331\n",
            "Epoch 69/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5576 - loss: 0.9227 - val_accuracy: 0.5386 - val_loss: 0.9413\n",
            "Epoch 70/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5543 - loss: 0.9213 - val_accuracy: 0.5399 - val_loss: 0.9308\n",
            "Epoch 71/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5532 - loss: 0.9185 - val_accuracy: 0.5396 - val_loss: 0.9284\n",
            "Epoch 72/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.5543 - loss: 0.9203 - val_accuracy: 0.5392 - val_loss: 0.9285\n",
            "Epoch 73/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5545 - loss: 0.9235 - val_accuracy: 0.5393 - val_loss: 0.9286\n",
            "Epoch 74/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5574 - loss: 0.9224 - val_accuracy: 0.5389 - val_loss: 0.9302\n",
            "Epoch 75/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5592 - loss: 0.9151 - val_accuracy: 0.5412 - val_loss: 0.9355\n",
            "Epoch 76/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5574 - loss: 0.9182 - val_accuracy: 0.5404 - val_loss: 0.9324\n",
            "Epoch 77/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5624 - loss: 0.9081 - val_accuracy: 0.5407 - val_loss: 0.9255\n",
            "Epoch 78/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5596 - loss: 0.9060 - val_accuracy: 0.5391 - val_loss: 0.9290\n",
            "Epoch 79/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5579 - loss: 0.9171 - val_accuracy: 0.5392 - val_loss: 0.9311\n",
            "Epoch 80/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5577 - loss: 0.9154 - val_accuracy: 0.5414 - val_loss: 0.9627\n",
            "Epoch 81/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5575 - loss: 0.9190 - val_accuracy: 0.5398 - val_loss: 0.9287\n",
            "Epoch 82/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5588 - loss: 0.9112 - val_accuracy: 0.5395 - val_loss: 0.9263\n",
            "Epoch 83/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5580 - loss: 0.9154 - val_accuracy: 0.5387 - val_loss: 0.9451\n",
            "Epoch 84/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5573 - loss: 0.9182 - val_accuracy: 0.5391 - val_loss: 0.9385\n",
            "Epoch 85/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5590 - loss: 0.9199 - val_accuracy: 0.5398 - val_loss: 0.9276\n",
            "Epoch 86/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5523 - loss: 0.9168 - val_accuracy: 0.5393 - val_loss: 0.9307\n",
            "Epoch 87/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.5573 - loss: 0.9082 - val_accuracy: 0.5387 - val_loss: 0.9567\n",
            "Epoch 88/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5548 - loss: 0.9165 - val_accuracy: 0.5401 - val_loss: 0.9332\n",
            "Epoch 89/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5566 - loss: 0.9142 - val_accuracy: 0.5397 - val_loss: 0.9304\n",
            "Epoch 90/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5587 - loss: 0.9061 - val_accuracy: 0.5392 - val_loss: 0.9304\n",
            "Epoch 91/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5547 - loss: 0.9200 - val_accuracy: 0.5387 - val_loss: 0.9272\n",
            "Epoch 92/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5557 - loss: 0.9152 - val_accuracy: 0.5397 - val_loss: 0.9297\n",
            "Epoch 93/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5542 - loss: 0.9157 - val_accuracy: 0.5390 - val_loss: 0.9330\n",
            "Epoch 94/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5561 - loss: 0.9150 - val_accuracy: 0.5397 - val_loss: 0.9222\n",
            "Epoch 95/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5561 - loss: 0.9147 - val_accuracy: 0.5387 - val_loss: 0.9474\n",
            "Epoch 96/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5592 - loss: 0.9050 - val_accuracy: 0.5389 - val_loss: 0.9386\n",
            "Epoch 97/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5540 - loss: 0.9151 - val_accuracy: 0.5400 - val_loss: 0.9333\n",
            "Epoch 98/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5529 - loss: 0.9178 - val_accuracy: 0.5400 - val_loss: 0.9239\n",
            "Epoch 99/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5579 - loss: 0.9111 - val_accuracy: 0.5392 - val_loss: 0.9241\n",
            "Epoch 100/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5533 - loss: 0.9124 - val_accuracy: 0.5388 - val_loss: 0.9307\n",
            "Epoch 101/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5596 - loss: 0.9174 - val_accuracy: 0.5391 - val_loss: 0.9260\n",
            "Epoch 102/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5557 - loss: 0.9137 - val_accuracy: 0.5422 - val_loss: 0.9653\n",
            "Epoch 103/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5579 - loss: 0.9193 - val_accuracy: 0.5394 - val_loss: 0.9265\n",
            "Epoch 104/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5522 - loss: 0.9153 - val_accuracy: 0.5388 - val_loss: 0.9404\n",
            "Epoch 105/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5555 - loss: 0.9099 - val_accuracy: 0.5414 - val_loss: 0.9662\n",
            "Epoch 106/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5579 - loss: 0.9095 - val_accuracy: 0.5401 - val_loss: 0.9241\n",
            "Epoch 107/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5566 - loss: 0.9190 - val_accuracy: 0.5402 - val_loss: 0.9363\n",
            "Epoch 108/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5579 - loss: 0.9130 - val_accuracy: 0.5395 - val_loss: 0.9294\n",
            "Epoch 109/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5566 - loss: 0.9142 - val_accuracy: 0.5390 - val_loss: 0.9336\n",
            "Epoch 110/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5559 - loss: 0.9161 - val_accuracy: 0.5417 - val_loss: 0.9202\n",
            "Epoch 111/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5492 - loss: 0.9132 - val_accuracy: 0.5391 - val_loss: 0.9271\n",
            "Epoch 112/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5562 - loss: 0.9115 - val_accuracy: 0.5398 - val_loss: 0.9273\n",
            "Epoch 113/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5549 - loss: 0.9156 - val_accuracy: 0.5401 - val_loss: 0.9580\n",
            "Epoch 114/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5608 - loss: 0.9057 - val_accuracy: 0.5396 - val_loss: 0.9226\n",
            "Epoch 115/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5524 - loss: 0.9159 - val_accuracy: 0.5403 - val_loss: 0.9298\n",
            "Epoch 116/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5586 - loss: 0.9165 - val_accuracy: 0.5387 - val_loss: 0.9337\n",
            "Epoch 117/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5550 - loss: 0.9145 - val_accuracy: 0.5401 - val_loss: 0.9215\n",
            "Epoch 118/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5587 - loss: 0.9060 - val_accuracy: 0.5387 - val_loss: 0.9597\n",
            "Epoch 119/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5550 - loss: 0.9153 - val_accuracy: 0.5411 - val_loss: 0.9205\n",
            "Epoch 120/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5563 - loss: 0.9101 - val_accuracy: 0.5404 - val_loss: 0.9404\n",
            "Epoch 121/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5564 - loss: 0.9155 - val_accuracy: 0.5389 - val_loss: 0.9282\n",
            "Epoch 122/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5554 - loss: 0.9159 - val_accuracy: 0.5403 - val_loss: 0.9267\n",
            "Epoch 123/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5584 - loss: 0.9134 - val_accuracy: 0.5409 - val_loss: 0.9276\n",
            "Epoch 124/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5550 - loss: 0.9093 - val_accuracy: 0.5387 - val_loss: 0.9212\n",
            "Epoch 125/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5582 - loss: 0.9074 - val_accuracy: 0.5404 - val_loss: 0.9204\n",
            "Epoch 126/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5594 - loss: 0.9129 - val_accuracy: 0.5398 - val_loss: 0.9245\n",
            "Epoch 127/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5575 - loss: 0.9139 - val_accuracy: 0.5389 - val_loss: 0.9262\n",
            "Epoch 128/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5574 - loss: 0.9150 - val_accuracy: 0.5397 - val_loss: 0.9235\n",
            "Epoch 129/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5591 - loss: 0.9091 - val_accuracy: 0.5399 - val_loss: 0.9337\n",
            "Epoch 130/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5554 - loss: 0.9096 - val_accuracy: 0.5390 - val_loss: 0.9219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set file paths\n",
        "zip_file_path = '/content/drive/My Drive/SP500_data2020-2024.zip'  # Replace with the path to the file in Google Drive\n",
        "output_dir = '/content/SP500_data2020-2024'  # Extract to Colab working directory\n",
        "\n",
        "# Extract files\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_dir)\n",
        "\n",
        "print(\"Files extracted to:\", output_dir)\n",
        "\n",
        "# List all CSV files in the extracted directory\n",
        "csv_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.csv')])\n",
        "print(f\"Found {len(csv_files)} CSV files.\")\n",
        "print(\"Sample files:\", csv_files[:5])  # Print the first 5 files alphabetically\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read all CSV files into a dictionary\n",
        "sp500_data = {}\n",
        "for csv_file in csv_files:\n",
        "    symbol = csv_file.replace('.csv', '')  # Extract the stock symbol\n",
        "    file_path = os.path.join(output_dir, csv_file)\n",
        "    sp500_data[symbol] = pd.read_csv(file_path)\n",
        "\n",
        "# Filter symbols to keep only those with the most common data length\n",
        "def filter_symbols_by_most_common_length(sp500_data):\n",
        "    # Calculate the length of data for each symbol\n",
        "    lengths = {symbol: len(data) for symbol, data in sp500_data.items()}\n",
        "\n",
        "    # Count the frequency of each length\n",
        "    length_counts = pd.Series(lengths).value_counts()\n",
        "\n",
        "    # Find the most common length\n",
        "    most_common_length = length_counts.idxmax()\n",
        "    print(f\"Most common length: {most_common_length}, Count: {length_counts.max()}\")\n",
        "\n",
        "    # Filter symbols with the most common length\n",
        "    filtered_symbols = [symbol for symbol, length in lengths.items() if length == most_common_length]\n",
        "\n",
        "    # Return filtered data and the most common length\n",
        "    filtered_data = {symbol: sp500_data[symbol] for symbol in filtered_symbols}\n",
        "    return filtered_data, most_common_length\n",
        "\n",
        "# Filter the data\n",
        "filtered_sp500_data, most_common_length = filter_symbols_by_most_common_length(sp500_data)\n",
        "\n",
        "# Check the filtered result\n",
        "print(f\"Filtered data count: {len(filtered_sp500_data)}\")\n",
        "\n",
        "# Window length\n",
        "window_size = 25\n",
        "degree = 2\n",
        "\n",
        "# Data collection\n",
        "data_set = {'matrices_4x4': [], 'labels1': [], 'labels2': [], 'labels3': []}\n",
        "\n",
        "# Create matrices and labels for each stock\n",
        "for index_name, data in filtered_sp500_data.items():\n",
        "    # Ensure 'Open' and 'Close' columns are numeric\n",
        "    data['Open'] = pd.to_numeric(data['Open'], errors='coerce')\n",
        "    data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
        "\n",
        "    # Extract 'Open' and 'Close' values, skipping the first two rows\n",
        "    open_values = data['Open'].values[2:]\n",
        "    close_values = data['Close'].values[2:]\n",
        "\n",
        "    # Build window matrices\n",
        "    num_samples = len(data) - window_size + 1\n",
        "    for start in range(num_samples - 2):  # Iterate through all data points\n",
        "        # Extract window data\n",
        "        open_row = open_values[start:start + window_size]\n",
        "        close_row = close_values[start:start + window_size]\n",
        "\n",
        "        # Normalize using the 16th last value of close_row\n",
        "        normalization_factor = close_row[-16]\n",
        "        open_row = open_row / normalization_factor\n",
        "        close_row = close_row / normalization_factor\n",
        "\n",
        "        # Create an alternating combined array\n",
        "        combined = np.array([open_row[i // 2] if i % 2 == 0 else close_row[i // 2] for i in range(window_size * 2)])\n",
        "        matrix_4x4 = combined[:-30].reshape(4, 5, 1)\n",
        "        result = [combined[-32], combined[-31], combined[-1]]\n",
        "\n",
        "        # Perform higher-order fitting on combined data\n",
        "        x = np.arange(len(combined[-4:-1]))\n",
        "        coeffs = poly.polyfit(x, result, deg=degree)\n",
        "\n",
        "        # Calculate first and second derivatives\n",
        "        first_derivative_coeffs = poly.polyder(coeffs)\n",
        "        first_derivatives = poly.polyval(x, first_derivative_coeffs)\n",
        "        second_derivative_coeffs = poly.polyder(first_derivative_coeffs)\n",
        "        second_derivatives = poly.polyval(x, second_derivative_coeffs)\n",
        "\n",
        "        # Calculate labels\n",
        "        label1 = 1 if first_derivatives[-1] > 0 else 0\n",
        "        label2 = 1 if second_derivatives[-1] > 0 else 0\n",
        "        label3 = 1 if close_row[-1] > close_row[-16] else 0\n",
        "\n",
        "        # Add to the unified data collection\n",
        "        data_set['matrices_4x4'].append(matrix_4x4)\n",
        "        data_set['labels1'].append(label1)\n",
        "        data_set['labels2'].append(label2)\n",
        "        data_set['labels3'].append(label3)\n",
        "\n",
        "# Convert data to NumPy arrays\n",
        "matrices_4x4 = np.array(data_set['matrices_4x4'])\n",
        "labels1 = np.array(data_set['labels1'])\n",
        "labels2 = np.array(data_set['labels2'])\n",
        "labels3 = np.array(data_set['labels3'])\n",
        "\n",
        "# Output results for verification\n",
        "print(f\"4x4 matrices shape: {matrices_4x4.shape}\")\n",
        "print(f\"Labels1 shape: {labels1.shape}\")\n",
        "print(f\"Labels2 shape: {labels2.shape}\")\n",
        "print(f\"Labels3 shape: {labels3.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K7d1ti7-x-M",
        "outputId": "f0182b26-00f4-4aa7-e8d3-b831e1b658c6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content/SP500_data2020-2024\n",
            "Found 501 CSV files.\n",
            "Sample files: ['A.csv', 'AAPL.csv', 'ABBV.csv', 'ABNB.csv', 'ABT.csv']\n",
            "Most common length: 1239, Count: 489\n",
            "Filtered data count: 489\n",
            "4x4 matrices shape: (593157, 4, 5, 1)\n",
            "Labels1 shape: (593157,)\n",
            "Labels2 shape: (593157,)\n",
            "Labels3 shape: (593157,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "\n",
        "# Get model prediction probabilities\n",
        "predictions2_pro = model1.predict(matrices_4x4)\n",
        "\n",
        "# Get predicted labels\n",
        "predicted_classes = np.argmax(predictions2_pro, axis=1)\n",
        "\n",
        "# Get the total number of classes\n",
        "num_classes = predictions2_pro.shape[1]\n",
        "\n",
        "# Initialize the final prediction array\n",
        "final_predictions = np.full(predictions2_pro.shape[0], -1)  # Initialize with -1 to indicate unclassified\n",
        "\n",
        "# Define the threshold range\n",
        "thresholds = np.arange(0.5, 0.95, 0.05)  # From 0.5 to 0.95 with a step of 0.05\n",
        "\n",
        "# Iterate over each threshold for evaluation\n",
        "for threshold in thresholds:\n",
        "    # Reset the prediction array\n",
        "    final_predictions[:] = -1\n",
        "\n",
        "    # Iterate over each class label to process samples predicted as that class\n",
        "    for class_label in range(num_classes):\n",
        "        # Find indices of samples predicted as the current class\n",
        "        class_indices = np.where(predicted_classes == class_label)[0]\n",
        "\n",
        "        if len(class_indices) > 0:\n",
        "            # Get probabilities for these samples\n",
        "            class_probs = predictions2_pro[class_indices, class_label]\n",
        "\n",
        "            # Filter indices where probability exceeds the threshold\n",
        "            selected_indices = class_indices[class_probs >= threshold]\n",
        "\n",
        "            # Mark as odd (1) or even (0)\n",
        "            odd_or_even = 1 if class_label % 2 == 1 else 0\n",
        "            final_predictions[selected_indices] = odd_or_even\n",
        "\n",
        "    # Filter out valid predictions\n",
        "    valid_indices = final_predictions != -1\n",
        "    filtered_final_predictions = final_predictions[valid_indices]\n",
        "    filtered_true_labels = labels3[valid_indices]  # Corresponding true labels\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(filtered_final_predictions == filtered_true_labels)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(filtered_true_labels, filtered_final_predictions, labels=[0, 1])\n",
        "\n",
        "    # Calculate Precision\n",
        "    tp = cm[1, 1]  # True Positives\n",
        "    fp = cm[0, 1]  # False Positives\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0  # Handle division by zero\n",
        "\n",
        "    # Calculate Recall\n",
        "    fn = cm[1, 0]  # False Negatives\n",
        "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0  # Handle division by zero\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Threshold: {threshold:.2f}\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "    print(f\"Precision (TP / (TP + FP)): {precision:.2%}\")\n",
        "    print(f\"Recall (TP / (TP + FN)): {recall:.2%}\")\n",
        "    print(f\"F1 Score: {f1:.2%}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t1sgBz1B1cR",
        "outputId": "fc311621-090d-45b5-ee09-659c48df20fa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18537/18537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step\n",
            "Threshold: 0.50\n",
            "Accuracy: 55.21%\n",
            "Precision (TP / (TP + FP)): 55.28%\n",
            "Recall (TP / (TP + FN)): 99.47%\n",
            "F1 Score: 71.07%\n",
            "Confusion Matrix:\n",
            "[[   885 205590]\n",
            " [  1351 254165]]\n",
            "\n",
            "Threshold: 0.55\n",
            "Accuracy: 55.32%\n",
            "Precision (TP / (TP + FP)): 55.33%\n",
            "Recall (TP / (TP + FN)): 99.89%\n",
            "F1 Score: 71.21%\n",
            "Confusion Matrix:\n",
            "[[   128 131094]\n",
            " [   171 162367]]\n",
            "\n",
            "Threshold: 0.60\n",
            "Accuracy: 64.69%\n",
            "Precision (TP / (TP + FP)): 64.71%\n",
            "Recall (TP / (TP + FN)): 99.39%\n",
            "F1 Score: 78.38%\n",
            "Confusion Matrix:\n",
            "[[  30 1604]\n",
            " [  18 2941]]\n",
            "\n",
            "Threshold: 0.65\n",
            "Accuracy: 67.74%\n",
            "Precision (TP / (TP + FP)): 67.67%\n",
            "Recall (TP / (TP + FN)): 99.90%\n",
            "F1 Score: 80.69%\n",
            "Confusion Matrix:\n",
            "[[  10  974]\n",
            " [   2 2039]]\n",
            "\n",
            "Threshold: 0.70\n",
            "Accuracy: 70.88%\n",
            "Precision (TP / (TP + FP)): 70.86%\n",
            "Recall (TP / (TP + FN)): 100.00%\n",
            "F1 Score: 82.95%\n",
            "Confusion Matrix:\n",
            "[[   1  602]\n",
            " [   0 1464]]\n",
            "\n",
            "Threshold: 0.75\n",
            "Accuracy: 74.11%\n",
            "Precision (TP / (TP + FP)): 74.11%\n",
            "Recall (TP / (TP + FN)): 100.00%\n",
            "F1 Score: 85.13%\n",
            "Confusion Matrix:\n",
            "[[   0  378]\n",
            " [   0 1082]]\n",
            "\n",
            "Threshold: 0.80\n",
            "Accuracy: 75.65%\n",
            "Precision (TP / (TP + FP)): 75.65%\n",
            "Recall (TP / (TP + FN)): 100.00%\n",
            "F1 Score: 86.14%\n",
            "Confusion Matrix:\n",
            "[[  0 242]\n",
            " [  0 752]]\n",
            "\n",
            "Threshold: 0.85\n",
            "Accuracy: 76.05%\n",
            "Precision (TP / (TP + FP)): 76.05%\n",
            "Recall (TP / (TP + FN)): 100.00%\n",
            "F1 Score: 86.40%\n",
            "Confusion Matrix:\n",
            "[[  0 148]\n",
            " [  0 470]]\n",
            "\n",
            "Threshold: 0.90\n",
            "Accuracy: 76.47%\n",
            "Precision (TP / (TP + FP)): 76.47%\n",
            "Recall (TP / (TP + FN)): 100.00%\n",
            "F1 Score: 86.67%\n",
            "Confusion Matrix:\n",
            "[[  0  84]\n",
            " [  0 273]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "# **Ensure input data is in 2D format**\n",
        "X_train = train_data[0].reshape(train_data[0].shape[0], -1)\n",
        "X_val = val_data[0].reshape(val_data[0].shape[0], -1)\n",
        "X_test = matrices_4x4.reshape(matrices_4x4.shape[0], -1)\n",
        "\n",
        "# **Combine X_train and X_val**\n",
        "X_train = np.vstack((X_train, X_val))  # Merge training and validation sets\n",
        "\n",
        "# **Ensure labels are integers**\n",
        "y_train = train_data[4].astype(int)\n",
        "y_val = val_data[4].astype(int)\n",
        "y_test = labels3.astype(int)\n",
        "\n",
        "# **Combine y_train and y_val**\n",
        "y_train = np.hstack((y_train, y_val))  # Merge labels of training and validation sets\n",
        "\n",
        "# **Initialize models**\n",
        "models = {\n",
        "    \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=42, n_estimators=100, verbose=1),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(random_state=42, n_estimators=100, verbose=1),\n",
        "    \"LogisticRegression\": LogisticRegression(random_state=42, max_iter=500, verbose=1),\n",
        "    \"XGBRegressor\": XGBRegressor(random_state=42, n_estimators=100, verbosity=1)\n",
        "}\n",
        "\n",
        "# **Set threshold range**\n",
        "thresholds = np.arange(0.5, 0.95, 0.05)  # From 0.5 to 0.95 with a step of 0.05\n",
        "\n",
        "# **Store results for all models**\n",
        "results = {}\n",
        "\n",
        "# **Iterate over each model**\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "\n",
        "    # **Train the model**\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # **Get predictions**\n",
        "    if model_name == \"XGBRegressor\":\n",
        "        # XGBRegressor outputs continuous values, need to convert to classification results\n",
        "        y_pred_test = model.predict(X_test)\n",
        "        prob_positive = y_pred_test  # Probability of positive class\n",
        "        prob_negative = 1 - y_pred_test  # Probability of negative class\n",
        "    else:\n",
        "        # Other models output probabilities\n",
        "        y_proba_test = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "        prob_positive = y_proba_test  # Probability of positive class\n",
        "        prob_negative = 1 - y_proba_test  # Probability of negative class\n",
        "\n",
        "    # **Store results for different thresholds**\n",
        "    results[model_name] = {}\n",
        "\n",
        "    # **Iterate over each threshold**\n",
        "    for threshold in thresholds:\n",
        "        # **Initialize final prediction array**\n",
        "        final_predictions = np.full(prob_positive.shape[0], -1)  # -1 indicates unclassified\n",
        "\n",
        "        # **Filter positive and negative classes**\n",
        "        positive_indices = prob_positive >= threshold  # Positive class probability >= threshold\n",
        "        negative_indices = prob_negative >= threshold  # Negative class probability >= threshold\n",
        "\n",
        "        # **Classify based on conditions**\n",
        "        final_predictions[positive_indices] = 1  # Positive class\n",
        "        final_predictions[negative_indices] = 0  # Negative class\n",
        "\n",
        "        # **Filter out valid predictions**\n",
        "        valid_indices = final_predictions != -1\n",
        "        filtered_final_predictions = final_predictions[valid_indices]\n",
        "        filtered_true_labels = y_test[valid_indices]  # True labels\n",
        "\n",
        "        # **Skip current threshold if no valid predictions**\n",
        "        if len(filtered_final_predictions) == 0:\n",
        "            print(f\"Threshold: {threshold:.2f} | No valid predictions.\")\n",
        "            continue\n",
        "\n",
        "        # **Calculate accuracy**\n",
        "        accuracy = np.mean(filtered_final_predictions == filtered_true_labels)\n",
        "\n",
        "        # **Create confusion matrix**\n",
        "        cm = confusion_matrix(filtered_true_labels, filtered_final_predictions, labels=[0, 1])\n",
        "\n",
        "        # **Calculate precision, avoiding division by zero**\n",
        "        tp = cm[1, 1]  # True Positives\n",
        "        fp = cm[0, 1]  # False Positives\n",
        "        precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "\n",
        "        # **Calculate recall, avoiding division by zero**\n",
        "        fn = cm[1, 0]  # False Negatives\n",
        "        recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "\n",
        "        # **Calculate F1 Score, avoiding division by zero**\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "        # **Store results**\n",
        "        results[model_name][threshold] = {\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1 Score\": f1,\n",
        "            \"Confusion Matrix\": cm\n",
        "        }\n",
        "\n",
        "        # **Print results**\n",
        "        print(f\"Model: {model_name} | Threshold: {threshold:.2f}\")\n",
        "        print(f\"  Accuracy: {accuracy:.2%}\")\n",
        "        print(f\"  Precision: {precision:.2%}\")\n",
        "        print(f\"  Recall: {recall:.2%}\")\n",
        "        print(f\"  F1 Score: {f1:.2%}\")\n",
        "        print(f\"  Confusion Matrix:\\n{cm}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blzz73FY_heH",
        "outputId": "ff408d11-acbe-4522-c8fc-b68f59a5c812"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training GradientBoostingClassifier...\n",
            "      Iter       Train Loss   Remaining Time \n",
            "         1           1.3569            1.56m\n",
            "         2           1.3566            1.54m\n",
            "         3           1.3564            1.52m\n",
            "         4           1.3562            1.50m\n",
            "         5           1.3560            1.49m\n",
            "         6           1.3558            1.56m\n",
            "         7           1.3556            1.60m\n",
            "         8           1.3553            1.63m\n",
            "         9           1.3552            1.59m\n",
            "        10           1.3549            1.55m\n",
            "        20           1.3529            1.36m\n",
            "        30           1.3514            1.20m\n",
            "        40           1.3502            1.03m\n",
            "        50           1.3486           50.99s\n",
            "        60           1.3473           40.32s\n",
            "        70           1.3458           30.26s\n",
            "        80           1.3446           20.17s\n",
            "        90           1.3433            9.96s\n",
            "       100           1.3422            0.00s\n",
            "Model: GradientBoostingClassifier | Threshold: 0.50\n",
            "  Accuracy: 54.93%\n",
            "  Precision: 55.17%\n",
            "  Recall: 98.49%\n",
            "  F1 Score: 70.72%\n",
            "  Confusion Matrix:\n",
            "[[  2879 262410]\n",
            " [  4947 322921]]\n",
            "\n",
            "Model: GradientBoostingClassifier | Threshold: 0.55\n",
            "  Accuracy: 54.89%\n",
            "  Precision: 54.98%\n",
            "  Recall: 99.48%\n",
            "  F1 Score: 70.82%\n",
            "  Confusion Matrix:\n",
            "[[   751 245021]\n",
            " [  1579 299271]]\n",
            "\n",
            "Model: GradientBoostingClassifier | Threshold: 0.60\n",
            "  Accuracy: 54.73%\n",
            "  Precision: 54.94%\n",
            "  Recall: 98.89%\n",
            "  F1 Score: 70.64%\n",
            "  Confusion Matrix:\n",
            "[[  250 42827]\n",
            " [  584 52224]]\n",
            "\n",
            "Model: GradientBoostingClassifier | Threshold: 0.65\n",
            "  Accuracy: 60.76%\n",
            "  Precision: 62.89%\n",
            "  Recall: 93.01%\n",
            "  F1 Score: 75.04%\n",
            "  Confusion Matrix:\n",
            "[[  99 1947]\n",
            " [ 248 3300]]\n",
            "\n",
            "Model: GradientBoostingClassifier | Threshold: 0.70\n",
            "  Accuracy: 65.00%\n",
            "  Precision: 67.08%\n",
            "  Recall: 94.01%\n",
            "  F1 Score: 78.29%\n",
            "  Confusion Matrix:\n",
            "[[  44  724]\n",
            " [  94 1475]]\n",
            "\n",
            "Model: GradientBoostingClassifier | Threshold: 0.75\n",
            "  Accuracy: 70.51%\n",
            "  Precision: 73.30%\n",
            "  Recall: 93.31%\n",
            "  F1 Score: 82.10%\n",
            "  Confusion Matrix:\n",
            "[[ 22 188]\n",
            " [ 37 516]]\n",
            "\n",
            "Model: GradientBoostingClassifier | Threshold: 0.80\n",
            "  Accuracy: 71.18%\n",
            "  Precision: 73.91%\n",
            "  Recall: 92.73%\n",
            "  F1 Score: 82.26%\n",
            "  Confusion Matrix:\n",
            "[[ 10  54]\n",
            " [ 12 153]]\n",
            "\n",
            "Model: GradientBoostingClassifier | Threshold: 0.85\n",
            "  Accuracy: 75.56%\n",
            "  Precision: 80.49%\n",
            "  Recall: 91.67%\n",
            "  F1 Score: 85.71%\n",
            "  Confusion Matrix:\n",
            "[[ 1  8]\n",
            " [ 3 33]]\n",
            "\n",
            "Model: GradientBoostingClassifier | Threshold: 0.90\n",
            "  Accuracy: 100.00%\n",
            "  Precision: 100.00%\n",
            "  Recall: 100.00%\n",
            "  F1 Score: 100.00%\n",
            "  Confusion Matrix:\n",
            "[[0 0]\n",
            " [0 4]]\n",
            "\n",
            "\n",
            "Training RandomForestClassifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   49.4s\n",
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: RandomForestClassifier | Threshold: 0.50\n",
            "  Accuracy: 52.95%\n",
            "  Precision: 55.11%\n",
            "  Recall: 80.23%\n",
            "  F1 Score: 65.34%\n",
            "  Confusion Matrix:\n",
            "[[ 51055 214234]\n",
            " [ 64823 263045]]\n",
            "\n",
            "Model: RandomForestClassifier | Threshold: 0.55\n",
            "  Accuracy: 53.97%\n",
            "  Precision: 54.99%\n",
            "  Recall: 90.64%\n",
            "  F1 Score: 68.45%\n",
            "  Confusion Matrix:\n",
            "[[ 16339 165868]\n",
            " [ 20937 202658]]\n",
            "\n",
            "Model: RandomForestClassifier | Threshold: 0.60\n",
            "  Accuracy: 54.52%\n",
            "  Precision: 54.89%\n",
            "  Recall: 96.53%\n",
            "  F1 Score: 69.99%\n",
            "  Confusion Matrix:\n",
            "[[ 2755 80924]\n",
            " [ 3540 98479]]\n",
            "\n",
            "Model: RandomForestClassifier | Threshold: 0.65\n",
            "  Accuracy: 54.53%\n",
            "  Precision: 54.71%\n",
            "  Recall: 98.30%\n",
            "  F1 Score: 70.30%\n",
            "  Confusion Matrix:\n",
            "[[  504 31202]\n",
            " [  651 37690]]\n",
            "\n",
            "Model: RandomForestClassifier | Threshold: 0.70\n",
            "  Accuracy: 54.60%\n",
            "  Precision: 54.74%\n",
            "  Recall: 98.75%\n",
            "  F1 Score: 70.43%\n",
            "  Confusion Matrix:\n",
            "[[  101  8557]\n",
            " [  131 10348]]\n",
            "\n",
            "Model: RandomForestClassifier | Threshold: 0.75\n",
            "  Accuracy: 54.55%\n",
            "  Precision: 54.67%\n",
            "  Recall: 98.82%\n",
            "  F1 Score: 70.39%\n",
            "  Confusion Matrix:\n",
            "[[  21 1802]\n",
            " [  26 2173]]\n",
            "\n",
            "Model: RandomForestClassifier | Threshold: 0.80\n",
            "  Accuracy: 57.45%\n",
            "  Precision: 57.31%\n",
            "  Recall: 99.49%\n",
            "  F1 Score: 72.73%\n",
            "  Confusion Matrix:\n",
            "[[  5 292]\n",
            " [  2 392]]\n",
            "\n",
            "Model: RandomForestClassifier | Threshold: 0.85\n",
            "  Accuracy: 51.58%\n",
            "  Precision: 51.58%\n",
            "  Recall: 100.00%\n",
            "  F1 Score: 68.06%\n",
            "  Confusion Matrix:\n",
            "[[ 0 46]\n",
            " [ 0 49]]\n",
            "\n",
            "Model: RandomForestClassifier | Threshold: 0.90\n",
            "  Accuracy: 50.00%\n",
            "  Precision: 50.00%\n",
            "  Recall: 100.00%\n",
            "  F1 Score: 66.67%\n",
            "  Confusion Matrix:\n",
            "[[0 1]\n",
            " [0 1]]\n",
            "\n",
            "\n",
            "Training LogisticRegression...\n",
            "Model: LogisticRegression | Threshold: 0.50\n",
            "  Accuracy: 55.28%\n",
            "  Precision: 55.28%\n",
            "  Recall: 100.00%\n",
            "  F1 Score: 71.20%\n",
            "  Confusion Matrix:\n",
            "[[     0 265289]\n",
            " [     0 327868]]\n",
            "\n",
            "Model: LogisticRegression | Threshold: 0.55\n",
            "  Accuracy: 55.28%\n",
            "  Precision: 55.28%\n",
            "  Recall: 100.00%\n",
            "  F1 Score: 71.20%\n",
            "  Confusion Matrix:\n",
            "[[     0 265289]\n",
            " [     0 327868]]\n",
            "\n",
            "Model: LogisticRegression | Threshold: 0.60\n",
            "  Accuracy: 67.89%\n",
            "  Precision: 67.89%\n",
            "  Recall: 100.00%\n",
            "  F1 Score: 80.88%\n",
            "  Confusion Matrix:\n",
            "[[   0  777]\n",
            " [   0 1643]]\n",
            "\n",
            "Model: LogisticRegression | Threshold: 0.65\n",
            "  Accuracy: 58.06%\n",
            "  Precision: 58.06%\n",
            "  Recall: 100.00%\n",
            "  F1 Score: 73.47%\n",
            "  Confusion Matrix:\n",
            "[[ 0 26]\n",
            " [ 0 36]]\n",
            "\n",
            "Model: LogisticRegression | Threshold: 0.70\n",
            "  Accuracy: 85.71%\n",
            "  Precision: 85.71%\n",
            "  Recall: 100.00%\n",
            "  F1 Score: 92.31%\n",
            "  Confusion Matrix:\n",
            "[[0 1]\n",
            " [0 6]]\n",
            "\n",
            "Model: LogisticRegression | Threshold: 0.75\n",
            "  Accuracy: 100.00%\n",
            "  Precision: 100.00%\n",
            "  Recall: 100.00%\n",
            "  F1 Score: 100.00%\n",
            "  Confusion Matrix:\n",
            "[[0 0]\n",
            " [0 1]]\n",
            "\n",
            "Threshold: 0.80 | No valid predictions.\n",
            "Threshold: 0.85 | No valid predictions.\n",
            "Threshold: 0.90 | No valid predictions.\n",
            "\n",
            "Training XGBRegressor...\n",
            "Model: XGBRegressor | Threshold: 0.50\n",
            "  Accuracy: 52.63%\n",
            "  Precision: 55.03%\n",
            "  Recall: 78.22%\n",
            "  F1 Score: 64.61%\n",
            "  Confusion Matrix:\n",
            "[[ 55684 209605]\n",
            " [ 71396 256472]]\n",
            "\n",
            "Model: XGBRegressor | Threshold: 0.55\n",
            "  Accuracy: 53.15%\n",
            "  Precision: 55.08%\n",
            "  Recall: 83.56%\n",
            "  F1 Score: 66.40%\n",
            "  Confusion Matrix:\n",
            "[[ 30775 168923]\n",
            " [ 40763 207145]]\n",
            "\n",
            "Model: XGBRegressor | Threshold: 0.60\n",
            "  Accuracy: 53.42%\n",
            "  Precision: 55.15%\n",
            "  Recall: 85.88%\n",
            "  F1 Score: 67.16%\n",
            "  Confusion Matrix:\n",
            "[[ 17046 114380]\n",
            " [ 23118 140622]]\n",
            "\n",
            "Model: XGBRegressor | Threshold: 0.65\n",
            "  Accuracy: 53.51%\n",
            "  Precision: 55.47%\n",
            "  Recall: 85.73%\n",
            "  F1 Score: 67.36%\n",
            "  Confusion Matrix:\n",
            "[[ 9287 64418]\n",
            " [13355 80238]]\n",
            "\n",
            "Model: XGBRegressor | Threshold: 0.70\n",
            "  Accuracy: 53.80%\n",
            "  Precision: 56.15%\n",
            "  Recall: 84.27%\n",
            "  F1 Score: 67.40%\n",
            "  Confusion Matrix:\n",
            "[[ 5240 32319]\n",
            " [ 7727 41389]]\n",
            "\n",
            "Model: XGBRegressor | Threshold: 0.75\n",
            "  Accuracy: 53.68%\n",
            "  Precision: 56.83%\n",
            "  Recall: 81.21%\n",
            "  F1 Score: 66.87%\n",
            "  Confusion Matrix:\n",
            "[[ 2956 15133]\n",
            " [ 4609 19925]]\n",
            "\n",
            "Model: XGBRegressor | Threshold: 0.80\n",
            "  Accuracy: 53.24%\n",
            "  Precision: 57.38%\n",
            "  Recall: 77.37%\n",
            "  F1 Score: 65.89%\n",
            "  Confusion Matrix:\n",
            "[[1675 6963]\n",
            " [2742 9375]]\n",
            "\n",
            "Model: XGBRegressor | Threshold: 0.85\n",
            "  Accuracy: 53.02%\n",
            "  Precision: 58.90%\n",
            "  Recall: 72.87%\n",
            "  F1 Score: 65.14%\n",
            "  Confusion Matrix:\n",
            "[[ 928 3114]\n",
            " [1662 4463]]\n",
            "\n",
            "Model: XGBRegressor | Threshold: 0.90\n",
            "  Accuracy: 52.54%\n",
            "  Precision: 59.86%\n",
            "  Recall: 67.55%\n",
            "  F1 Score: 63.47%\n",
            "  Confusion Matrix:\n",
            "[[ 574 1403]\n",
            " [1005 2092]]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}