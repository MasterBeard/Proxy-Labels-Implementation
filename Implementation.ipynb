{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGmKbOYzQhq1PSU6R0hFN4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasterBeard/Proxy-Labels-Implementation/blob/main/Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure the reproducibility of the article, we have provided a notebook code that explains how to use proxy labels to identify investment portfolios with higher precision and a greater number of correct upward signal predictions. The example in the notebook code involves the test results for **Test Set 2** under **Task 1**."
      ],
      "metadata": {
        "id": "q9X4PHmFNzUT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUz5xqM49LWU",
        "outputId": "4cb56409-e052-4e55-a763-265e63a4a749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 4x4 matrices shape: (44871, 4, 5, 1)\n",
            "Train 1st matrices shape: (44871, 32)\n",
            "Train labels1 shape: (44871,)\n",
            "Train origin shape: (44871,)\n",
            "Validation 4x4 matrices shape: (23353, 4, 5, 1)\n",
            "Validation 1st matrices shape: (23353, 32)\n",
            "Validation labels1 shape: (23353,)\n",
            "Validation origin shape: (23353,)\n",
            "Test 4x4 matrices shape: (22604, 4, 5, 1)\n",
            "Test 1st matrices shape: (22604, 32)\n",
            "Test labels1 shape: (22604,)\n",
            "Test origin shape: (22604,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy.polynomial.polynomial as poly\n",
        "\n",
        "# Define stock and index tickers\n",
        "stock_tickers = [\n",
        "    'OKE', 'ENPH', 'UHS', 'DLTR', 'AMZN', 'EFX', 'RSG', 'OXY', 'REGN', 'DECK',\n",
        "    '^GSPC',  # S&P 500\n",
        "    '^IXIC',  # NASDAQ Composite\n",
        "    '^HSI',   # Hang Seng Index\n",
        "    '^DJI',   # Dow Jones Industrial Average\n",
        "    '^FCHI',  # CAC 40\n",
        "    '^GDAXI', # DAX\n",
        "    '^N225',  # Nikkei 225\n",
        "    '^KS11',  # KOSPI\n",
        "    '^STOXX50E'  # EURO STOXX 50\n",
        "]\n",
        "\n",
        "# Define date ranges\n",
        "date_ranges = {\n",
        "    'train': (\"2005-01-01\", \"2015-01-01\"),\n",
        "    'val': (\"2015-01-02\", \"2019-12-31\"),\n",
        "    'test': (\"2020-01-01\", \"2024-10-31\")\n",
        "}\n",
        "\n",
        "# Store matrices, labels, and origin for each split\n",
        "data_splits = {\n",
        "    split: {\n",
        "        'matrices_4x4': [],\n",
        "        'matrices_1st': [],\n",
        "        'labels4': [],\n",
        "        'labels1': [],\n",
        "        'labels2': [],\n",
        "        'labels3': [],\n",
        "        'origin': []  # Record the origin (stock or index) of each matrices_4x4\n",
        "    }\n",
        "    for split in date_ranges\n",
        "}\n",
        "\n",
        "# Window length\n",
        "window_size = 25\n",
        "degree = 2\n",
        "\n",
        "# Fetch and process data for each time period\n",
        "for split, (start_date, end_date) in date_ranges.items():\n",
        "    # Download all data (stocks and indices) for the specified date range\n",
        "    all_data = {ticker: yf.download(ticker, start=start_date, end=end_date, auto_adjust=False) for ticker in stock_tickers}\n",
        "\n",
        "    # Create first and second derivative matrices for each stock and index\n",
        "    for ticker, data in all_data.items():\n",
        "        # Extract required 'Open' and 'Close' data\n",
        "        open_values = data['Open'].dropna().values\n",
        "        close_values = data['Close'].dropna().values\n",
        "\n",
        "        # Build window matrices\n",
        "        for start in range(len(data) - window_size + 1):\n",
        "            # Extract each row of data\n",
        "            open_row = open_values[start:start + window_size]\n",
        "            close_row = close_values[start:start + window_size]\n",
        "\n",
        "            # Normalize using the 16th last value of close_row\n",
        "            normalization_factor = close_row[-16]\n",
        "            open_row = open_row / normalization_factor\n",
        "            close_row = close_row / normalization_factor\n",
        "\n",
        "            # Create an alternating combined array\n",
        "            combined = np.array([open_row[i // 2] if i % 2 == 0 else close_row[i // 2] for i in range(window_size * 2)])\n",
        "            matrix_4x4 = combined[:-30].reshape(4, 5, 1)\n",
        "            matrix_4x5 = combined[-32:].reshape(-1)\n",
        "            result = [combined[-32], combined[-31], combined[-1]]\n",
        "            data_splits[split]['matrices_4x4'].append(matrix_4x4)\n",
        "            data_splits[split]['matrices_1st'].append(matrix_4x5)\n",
        "            data_splits[split]['origin'].append(ticker)  # Record origin (stock or index)\n",
        "\n",
        "            # Perform higher-order fitting on combined data\n",
        "            x = np.arange(len(combined[-4:-1]))\n",
        "            coeffs = poly.polyfit(x, result, deg=degree)\n",
        "\n",
        "            # Calculate first derivative\n",
        "            first_derivative_coeffs = poly.polyder(coeffs)\n",
        "            first_derivatives = poly.polyval(x, first_derivative_coeffs)\n",
        "\n",
        "            # Calculate second derivative\n",
        "            second_derivative_coeffs = poly.polyder(first_derivative_coeffs)\n",
        "            second_derivatives = poly.polyval(x, second_derivative_coeffs)\n",
        "\n",
        "            # Generate labels\n",
        "            label1 = 1 if first_derivatives[0][-1] > 0 else 0\n",
        "            label2 = 1 if second_derivatives[0][-1] > 0 else 0\n",
        "            label3 = 1 if close_row[-1] > close_row[-16] else 0\n",
        "            label4 = [first_derivatives[-1], second_derivatives[-1]]\n",
        "\n",
        "            data_splits[split]['labels1'].append(label1)\n",
        "            data_splits[split]['labels2'].append(label2)\n",
        "            data_splits[split]['labels3'].append(label3)\n",
        "            data_splits[split]['labels4'].append(label4)\n",
        "\n",
        "# Convert matrices in each split to NumPy arrays\n",
        "def convert_to_numpy(split):\n",
        "    return (\n",
        "        np.array(data_splits[split]['matrices_4x4']),\n",
        "        np.array(data_splits[split]['matrices_1st']),\n",
        "        np.array(data_splits[split]['labels1']),\n",
        "        np.array(data_splits[split]['labels2']),\n",
        "        np.array(data_splits[split]['labels3']),\n",
        "        np.array(data_splits[split]['labels4']),\n",
        "        np.array(data_splits[split]['origin'])\n",
        "    )\n",
        "\n",
        "train_data = convert_to_numpy('train')\n",
        "val_data = convert_to_numpy('val')\n",
        "test_data = convert_to_numpy('test')\n",
        "\n",
        "# Output shapes of each split to check results\n",
        "print(f\"Train 4x4 matrices shape: {train_data[0].shape}\")\n",
        "print(f\"Train 1st matrices shape: {train_data[1].shape}\")\n",
        "print(f\"Train labels1 shape: {train_data[2].shape}\")\n",
        "print(f\"Train origin shape: {train_data[6].shape}\")\n",
        "\n",
        "print(f\"Validation 4x4 matrices shape: {val_data[0].shape}\")\n",
        "print(f\"Validation 1st matrices shape: {val_data[1].shape}\")\n",
        "print(f\"Validation labels1 shape: {val_data[2].shape}\")\n",
        "print(f\"Validation origin shape: {val_data[6].shape}\")\n",
        "\n",
        "print(f\"Test 4x4 matrices shape: {test_data[0].shape}\")\n",
        "print(f\"Test 1st matrices shape: {test_data[1].shape}\")\n",
        "print(f\"Test labels1 shape: {test_data[2].shape}\")\n",
        "print(f\"Test origin shape: {test_data[6].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_labels(labels1, labels2, labels3):\n",
        "    \"\"\"\n",
        "    Generate new grouped labels based on the permutations of the three input labels.\n",
        "    Each combination is uniquely encoded: e.g., (0, 1, 1) → 3.\n",
        "    \"\"\"\n",
        "    # Combine labels into a unique code using weighted sum: labels1 * 4 + labels2 * 2 + labels3 * 1\n",
        "    combined_labels = labels1 * 4 + labels2 * 2 + labels3 * 1\n",
        "    return combined_labels\n",
        "\n",
        "# Regenerate labels for train, validation, and test sets\n",
        "train_labels = combine_labels(train_data[2], train_data[3], train_data[4])\n",
        "val_labels = combine_labels(val_data[2], val_data[3], val_data[4])\n",
        "test_labels = combine_labels(test_data[2], test_data[3], test_data[4])\n",
        "\n",
        "# Print label distributions to check the combinations\n",
        "print(\"Train Labels Distribution:\", np.unique(train_labels, return_counts=True))\n",
        "print(\"Validation Labels Distribution:\", np.unique(val_labels, return_counts=True))\n",
        "print(\"Test Labels Distribution:\", np.unique(test_labels, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSJ_Cs0U-CNI",
        "outputId": "84a0b7b3-2adf-4b2d-8b85-8c7492c4d856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Labels Distribution: (array([0, 1, 2, 5, 6, 7]), array([17051,   559,   859,  1096,   438, 24868]))\n",
            "Validation Labels Distribution: (array([0, 1, 2, 5, 6, 7]), array([ 9210,   263,   480,   560,   264, 12576]))\n",
            "Test Labels Distribution: (array([0, 1, 2, 5, 6, 7]), array([ 9010,   241,   446,   492,   225, 12190]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Build MLP model\n",
        "model1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(4, 5, 1)),  # Flatten (4, 5, 1) into (20,)\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),  # First Dense layer\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),  # Second Dense layer\n",
        "    tf.keras.layers.Dense(units=8, activation='softmax')  # 8-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model1.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',  # Suitable for integer labels\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history1 = model1.fit(\n",
        "    train_data[0], train_labels,  # Training data (inputs, labels)\n",
        "    epochs=300,\n",
        "    batch_size=128,\n",
        "    validation_data=(val_data[0], val_labels),  # Validation data\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save model1\n",
        "model1.save('modeltaskmlp1.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzOf4Nnt9wDj",
        "outputId": "9282f26f-8bf3-4c5c-8f71-87a940c0d4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5350 - loss: 1.0502 - val_accuracy: 0.5385 - val_loss: 0.9795\n",
            "Epoch 2/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5562 - loss: 0.9645 - val_accuracy: 0.5386 - val_loss: 0.9755\n",
            "Epoch 3/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5545 - loss: 0.9628 - val_accuracy: 0.5385 - val_loss: 0.9729\n",
            "Epoch 4/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5601 - loss: 0.9605 - val_accuracy: 0.5385 - val_loss: 0.9728\n",
            "Epoch 5/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5522 - loss: 0.9692 - val_accuracy: 0.5385 - val_loss: 0.9729\n",
            "Epoch 6/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5547 - loss: 0.9648 - val_accuracy: 0.5385 - val_loss: 0.9736\n",
            "Epoch 7/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5501 - loss: 0.9660 - val_accuracy: 0.5385 - val_loss: 0.9718\n",
            "Epoch 8/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5545 - loss: 0.9641 - val_accuracy: 0.5385 - val_loss: 0.9757\n",
            "Epoch 9/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5509 - loss: 0.9613 - val_accuracy: 0.5385 - val_loss: 0.9731\n",
            "Epoch 10/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5555 - loss: 0.9637 - val_accuracy: 0.5385 - val_loss: 0.9718\n",
            "Epoch 11/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5524 - loss: 0.9666 - val_accuracy: 0.5385 - val_loss: 0.9734\n",
            "Epoch 12/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5557 - loss: 0.9660 - val_accuracy: 0.5385 - val_loss: 0.9712\n",
            "Epoch 13/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5532 - loss: 0.9637 - val_accuracy: 0.5385 - val_loss: 0.9757\n",
            "Epoch 14/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5517 - loss: 0.9643 - val_accuracy: 0.5385 - val_loss: 0.9706\n",
            "Epoch 15/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5535 - loss: 0.9630 - val_accuracy: 0.5385 - val_loss: 0.9747\n",
            "Epoch 16/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5575 - loss: 0.9582 - val_accuracy: 0.5385 - val_loss: 0.9710\n",
            "Epoch 17/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5525 - loss: 0.9621 - val_accuracy: 0.5385 - val_loss: 0.9719\n",
            "Epoch 18/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5516 - loss: 0.9674 - val_accuracy: 0.5385 - val_loss: 0.9719\n",
            "Epoch 19/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5545 - loss: 0.9602 - val_accuracy: 0.5385 - val_loss: 0.9791\n",
            "Epoch 20/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5549 - loss: 0.9590 - val_accuracy: 0.5385 - val_loss: 0.9696\n",
            "Epoch 21/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5553 - loss: 0.9546 - val_accuracy: 0.5386 - val_loss: 0.9702\n",
            "Epoch 22/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5527 - loss: 0.9635 - val_accuracy: 0.5386 - val_loss: 0.9750\n",
            "Epoch 23/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5563 - loss: 0.9600 - val_accuracy: 0.5386 - val_loss: 0.9672\n",
            "Epoch 24/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5514 - loss: 0.9628 - val_accuracy: 0.5386 - val_loss: 0.9671\n",
            "Epoch 25/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5554 - loss: 0.9545 - val_accuracy: 0.5386 - val_loss: 0.9698\n",
            "Epoch 26/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5570 - loss: 0.9496 - val_accuracy: 0.5386 - val_loss: 0.9665\n",
            "Epoch 27/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5572 - loss: 0.9535 - val_accuracy: 0.5386 - val_loss: 0.9672\n",
            "Epoch 28/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5565 - loss: 0.9507 - val_accuracy: 0.5386 - val_loss: 0.9632\n",
            "Epoch 29/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5559 - loss: 0.9535 - val_accuracy: 0.5386 - val_loss: 0.9635\n",
            "Epoch 30/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5520 - loss: 0.9506 - val_accuracy: 0.5385 - val_loss: 0.9633\n",
            "Epoch 31/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5551 - loss: 0.9536 - val_accuracy: 0.5385 - val_loss: 0.9595\n",
            "Epoch 32/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5522 - loss: 0.9413 - val_accuracy: 0.5384 - val_loss: 0.9653\n",
            "Epoch 33/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5582 - loss: 0.9448 - val_accuracy: 0.5385 - val_loss: 0.9564\n",
            "Epoch 34/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5514 - loss: 0.9471 - val_accuracy: 0.5383 - val_loss: 0.9575\n",
            "Epoch 35/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5492 - loss: 0.9482 - val_accuracy: 0.5384 - val_loss: 0.9522\n",
            "Epoch 36/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5543 - loss: 0.9409 - val_accuracy: 0.5389 - val_loss: 0.9507\n",
            "Epoch 37/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5544 - loss: 0.9437 - val_accuracy: 0.5385 - val_loss: 0.9549\n",
            "Epoch 38/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5535 - loss: 0.9432 - val_accuracy: 0.5386 - val_loss: 0.9569\n",
            "Epoch 39/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5527 - loss: 0.9353 - val_accuracy: 0.5386 - val_loss: 0.9499\n",
            "Epoch 40/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5537 - loss: 0.9375 - val_accuracy: 0.5384 - val_loss: 0.9636\n",
            "Epoch 41/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5571 - loss: 0.9303 - val_accuracy: 0.5392 - val_loss: 0.9519\n",
            "Epoch 42/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5581 - loss: 0.9315 - val_accuracy: 0.5386 - val_loss: 0.9550\n",
            "Epoch 43/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5552 - loss: 0.9381 - val_accuracy: 0.5384 - val_loss: 0.9465\n",
            "Epoch 44/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5568 - loss: 0.9257 - val_accuracy: 0.5388 - val_loss: 0.9422\n",
            "Epoch 45/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5578 - loss: 0.9318 - val_accuracy: 0.5386 - val_loss: 0.9588\n",
            "Epoch 46/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5577 - loss: 0.9321 - val_accuracy: 0.5387 - val_loss: 0.9405\n",
            "Epoch 47/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5548 - loss: 0.9295 - val_accuracy: 0.5386 - val_loss: 0.9490\n",
            "Epoch 48/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5541 - loss: 0.9337 - val_accuracy: 0.5385 - val_loss: 0.9438\n",
            "Epoch 49/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5534 - loss: 0.9311 - val_accuracy: 0.5391 - val_loss: 0.9414\n",
            "Epoch 50/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5613 - loss: 0.9237 - val_accuracy: 0.5394 - val_loss: 0.9452\n",
            "Epoch 51/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5546 - loss: 0.9293 - val_accuracy: 0.5396 - val_loss: 0.9433\n",
            "Epoch 52/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5569 - loss: 0.9241 - val_accuracy: 0.5400 - val_loss: 0.9417\n",
            "Epoch 53/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5542 - loss: 0.9317 - val_accuracy: 0.5385 - val_loss: 0.9398\n",
            "Epoch 54/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5548 - loss: 0.9287 - val_accuracy: 0.5393 - val_loss: 0.9400\n",
            "Epoch 55/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5545 - loss: 0.9229 - val_accuracy: 0.5398 - val_loss: 0.9381\n",
            "Epoch 56/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5569 - loss: 0.9183 - val_accuracy: 0.5385 - val_loss: 0.9441\n",
            "Epoch 57/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5576 - loss: 0.9214 - val_accuracy: 0.5393 - val_loss: 0.9337\n",
            "Epoch 58/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5534 - loss: 0.9338 - val_accuracy: 0.5389 - val_loss: 0.9359\n",
            "Epoch 59/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5554 - loss: 0.9217 - val_accuracy: 0.5409 - val_loss: 0.9413\n",
            "Epoch 60/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5576 - loss: 0.9224 - val_accuracy: 0.5384 - val_loss: 0.9337\n",
            "Epoch 61/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5544 - loss: 0.9245 - val_accuracy: 0.5385 - val_loss: 0.9465\n",
            "Epoch 62/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5581 - loss: 0.9215 - val_accuracy: 0.5401 - val_loss: 0.9387\n",
            "Epoch 63/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5548 - loss: 0.9197 - val_accuracy: 0.5394 - val_loss: 0.9331\n",
            "Epoch 64/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5597 - loss: 0.9200 - val_accuracy: 0.5395 - val_loss: 0.9301\n",
            "Epoch 65/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5562 - loss: 0.9271 - val_accuracy: 0.5398 - val_loss: 0.9319\n",
            "Epoch 66/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5555 - loss: 0.9230 - val_accuracy: 0.5394 - val_loss: 0.9410\n",
            "Epoch 67/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5578 - loss: 0.9203 - val_accuracy: 0.5393 - val_loss: 0.9348\n",
            "Epoch 68/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5537 - loss: 0.9189 - val_accuracy: 0.5406 - val_loss: 0.9381\n",
            "Epoch 69/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5614 - loss: 0.9141 - val_accuracy: 0.5392 - val_loss: 0.9301\n",
            "Epoch 70/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5568 - loss: 0.9174 - val_accuracy: 0.5393 - val_loss: 0.9293\n",
            "Epoch 71/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5600 - loss: 0.9149 - val_accuracy: 0.5395 - val_loss: 0.9311\n",
            "Epoch 72/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5553 - loss: 0.9171 - val_accuracy: 0.5393 - val_loss: 0.9365\n",
            "Epoch 73/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5573 - loss: 0.9213 - val_accuracy: 0.5395 - val_loss: 0.9283\n",
            "Epoch 74/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5564 - loss: 0.9155 - val_accuracy: 0.5391 - val_loss: 0.9333\n",
            "Epoch 75/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5526 - loss: 0.9243 - val_accuracy: 0.5398 - val_loss: 0.9322\n",
            "Epoch 76/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5543 - loss: 0.9241 - val_accuracy: 0.5393 - val_loss: 0.9305\n",
            "Epoch 77/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5612 - loss: 0.9158 - val_accuracy: 0.5400 - val_loss: 0.9379\n",
            "Epoch 78/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5587 - loss: 0.9230 - val_accuracy: 0.5401 - val_loss: 0.9268\n",
            "Epoch 79/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5547 - loss: 0.9216 - val_accuracy: 0.5392 - val_loss: 0.9310\n",
            "Epoch 80/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5524 - loss: 0.9225 - val_accuracy: 0.5403 - val_loss: 0.9309\n",
            "Epoch 81/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5545 - loss: 0.9177 - val_accuracy: 0.5399 - val_loss: 0.9279\n",
            "Epoch 82/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5574 - loss: 0.9103 - val_accuracy: 0.5398 - val_loss: 0.9258\n",
            "Epoch 83/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5566 - loss: 0.9125 - val_accuracy: 0.5398 - val_loss: 0.9342\n",
            "Epoch 84/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5542 - loss: 0.9221 - val_accuracy: 0.5392 - val_loss: 0.9338\n",
            "Epoch 85/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5535 - loss: 0.9180 - val_accuracy: 0.5392 - val_loss: 0.9266\n",
            "Epoch 86/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5584 - loss: 0.9142 - val_accuracy: 0.5408 - val_loss: 0.9278\n",
            "Epoch 87/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5517 - loss: 0.9161 - val_accuracy: 0.5394 - val_loss: 0.9338\n",
            "Epoch 88/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5536 - loss: 0.9140 - val_accuracy: 0.5413 - val_loss: 0.9279\n",
            "Epoch 89/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5552 - loss: 0.9185 - val_accuracy: 0.5390 - val_loss: 0.9400\n",
            "Epoch 90/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5572 - loss: 0.9120 - val_accuracy: 0.5395 - val_loss: 0.9274\n",
            "Epoch 91/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5552 - loss: 0.9145 - val_accuracy: 0.5393 - val_loss: 0.9308\n",
            "Epoch 92/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5552 - loss: 0.9150 - val_accuracy: 0.5405 - val_loss: 0.9271\n",
            "Epoch 93/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5576 - loss: 0.9118 - val_accuracy: 0.5401 - val_loss: 0.9284\n",
            "Epoch 94/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5577 - loss: 0.9159 - val_accuracy: 0.5397 - val_loss: 0.9253\n",
            "Epoch 95/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5546 - loss: 0.9133 - val_accuracy: 0.5394 - val_loss: 0.9321\n",
            "Epoch 96/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5544 - loss: 0.9145 - val_accuracy: 0.5397 - val_loss: 0.9242\n",
            "Epoch 97/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5537 - loss: 0.9168 - val_accuracy: 0.5394 - val_loss: 0.9284\n",
            "Epoch 98/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5575 - loss: 0.9233 - val_accuracy: 0.5394 - val_loss: 0.9247\n",
            "Epoch 99/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5582 - loss: 0.9174 - val_accuracy: 0.5409 - val_loss: 0.9288\n",
            "Epoch 100/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5541 - loss: 0.9157 - val_accuracy: 0.5390 - val_loss: 0.9329\n",
            "Epoch 101/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5564 - loss: 0.9153 - val_accuracy: 0.5392 - val_loss: 0.9379\n",
            "Epoch 102/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5600 - loss: 0.9115 - val_accuracy: 0.5393 - val_loss: 0.9382\n",
            "Epoch 103/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5548 - loss: 0.9137 - val_accuracy: 0.5395 - val_loss: 0.9249\n",
            "Epoch 104/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5578 - loss: 0.9120 - val_accuracy: 0.5393 - val_loss: 0.9236\n",
            "Epoch 105/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5556 - loss: 0.9112 - val_accuracy: 0.5407 - val_loss: 0.9317\n",
            "Epoch 106/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5546 - loss: 0.9129 - val_accuracy: 0.5407 - val_loss: 0.9220\n",
            "Epoch 107/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5577 - loss: 0.9132 - val_accuracy: 0.5416 - val_loss: 0.9402\n",
            "Epoch 108/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5570 - loss: 0.9159 - val_accuracy: 0.5406 - val_loss: 0.9376\n",
            "Epoch 109/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5505 - loss: 0.9125 - val_accuracy: 0.5391 - val_loss: 0.9407\n",
            "Epoch 110/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5561 - loss: 0.9204 - val_accuracy: 0.5391 - val_loss: 0.9250\n",
            "Epoch 111/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5573 - loss: 0.9073 - val_accuracy: 0.5393 - val_loss: 0.9292\n",
            "Epoch 112/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5577 - loss: 0.9088 - val_accuracy: 0.5390 - val_loss: 0.9849\n",
            "Epoch 113/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5547 - loss: 0.9181 - val_accuracy: 0.5401 - val_loss: 0.9243\n",
            "Epoch 114/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5564 - loss: 0.9140 - val_accuracy: 0.5407 - val_loss: 0.9258\n",
            "Epoch 115/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5550 - loss: 0.9153 - val_accuracy: 0.5393 - val_loss: 0.9350\n",
            "Epoch 116/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5535 - loss: 0.9172 - val_accuracy: 0.5387 - val_loss: 0.9683\n",
            "Epoch 117/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5552 - loss: 0.9177 - val_accuracy: 0.5407 - val_loss: 0.9253\n",
            "Epoch 118/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5609 - loss: 0.9046 - val_accuracy: 0.5387 - val_loss: 0.9397\n",
            "Epoch 119/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5560 - loss: 0.9148 - val_accuracy: 0.5389 - val_loss: 0.9301\n",
            "Epoch 120/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5597 - loss: 0.9069 - val_accuracy: 0.5407 - val_loss: 0.9203\n",
            "Epoch 121/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5562 - loss: 0.9190 - val_accuracy: 0.5393 - val_loss: 0.9231\n",
            "Epoch 122/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5570 - loss: 0.9080 - val_accuracy: 0.5420 - val_loss: 0.9252\n",
            "Epoch 123/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5546 - loss: 0.9112 - val_accuracy: 0.5407 - val_loss: 0.9291\n",
            "Epoch 124/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5574 - loss: 0.9057 - val_accuracy: 0.5412 - val_loss: 0.9468\n",
            "Epoch 125/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5577 - loss: 0.9059 - val_accuracy: 0.5415 - val_loss: 0.9275\n",
            "Epoch 126/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5532 - loss: 0.9150 - val_accuracy: 0.5406 - val_loss: 0.9230\n",
            "Epoch 127/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5512 - loss: 0.9218 - val_accuracy: 0.5395 - val_loss: 0.9238\n",
            "Epoch 128/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5590 - loss: 0.9033 - val_accuracy: 0.5405 - val_loss: 0.9198\n",
            "Epoch 129/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5577 - loss: 0.9050 - val_accuracy: 0.5383 - val_loss: 0.9464\n",
            "Epoch 130/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5579 - loss: 0.9111 - val_accuracy: 0.5397 - val_loss: 0.9203\n",
            "Epoch 131/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5558 - loss: 0.9066 - val_accuracy: 0.5414 - val_loss: 0.9341\n",
            "Epoch 132/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5566 - loss: 0.9154 - val_accuracy: 0.5427 - val_loss: 0.9963\n",
            "Epoch 133/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5553 - loss: 0.9177 - val_accuracy: 0.5396 - val_loss: 0.9220\n",
            "Epoch 134/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5546 - loss: 0.9134 - val_accuracy: 0.5391 - val_loss: 0.9238\n",
            "Epoch 135/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5531 - loss: 0.9116 - val_accuracy: 0.5392 - val_loss: 0.9218\n",
            "Epoch 136/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5591 - loss: 0.9036 - val_accuracy: 0.5402 - val_loss: 0.9332\n",
            "Epoch 137/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5639 - loss: 0.9010 - val_accuracy: 0.5392 - val_loss: 0.9448\n",
            "Epoch 138/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5588 - loss: 0.9090 - val_accuracy: 0.5411 - val_loss: 0.9482\n",
            "Epoch 139/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5568 - loss: 0.9108 - val_accuracy: 0.5395 - val_loss: 0.9234\n",
            "Epoch 140/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5574 - loss: 0.9115 - val_accuracy: 0.5414 - val_loss: 0.9265\n",
            "Epoch 141/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5530 - loss: 0.9063 - val_accuracy: 0.5393 - val_loss: 0.9205\n",
            "Epoch 142/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5541 - loss: 0.9106 - val_accuracy: 0.5400 - val_loss: 0.9234\n",
            "Epoch 143/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5601 - loss: 0.9025 - val_accuracy: 0.5412 - val_loss: 0.9359\n",
            "Epoch 144/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5538 - loss: 0.9144 - val_accuracy: 0.5392 - val_loss: 0.9192\n",
            "Epoch 145/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5594 - loss: 0.9081 - val_accuracy: 0.5387 - val_loss: 0.9771\n",
            "Epoch 146/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5539 - loss: 0.9206 - val_accuracy: 0.5404 - val_loss: 0.9202\n",
            "Epoch 147/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5563 - loss: 0.9082 - val_accuracy: 0.5398 - val_loss: 0.9205\n",
            "Epoch 148/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5586 - loss: 0.9120 - val_accuracy: 0.5391 - val_loss: 0.9316\n",
            "Epoch 149/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5572 - loss: 0.9086 - val_accuracy: 0.5396 - val_loss: 0.9195\n",
            "Epoch 150/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5554 - loss: 0.9095 - val_accuracy: 0.5393 - val_loss: 0.9300\n",
            "Epoch 151/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5598 - loss: 0.9123 - val_accuracy: 0.5406 - val_loss: 0.9207\n",
            "Epoch 152/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5579 - loss: 0.9034 - val_accuracy: 0.5417 - val_loss: 0.9442\n",
            "Epoch 153/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5562 - loss: 0.9153 - val_accuracy: 0.5384 - val_loss: 0.9451\n",
            "Epoch 154/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5529 - loss: 0.9157 - val_accuracy: 0.5401 - val_loss: 0.9225\n",
            "Epoch 155/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5569 - loss: 0.9090 - val_accuracy: 0.5393 - val_loss: 0.9215\n",
            "Epoch 156/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5596 - loss: 0.9044 - val_accuracy: 0.5393 - val_loss: 0.9202\n",
            "Epoch 157/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5588 - loss: 0.9029 - val_accuracy: 0.5419 - val_loss: 0.9607\n",
            "Epoch 158/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5560 - loss: 0.9050 - val_accuracy: 0.5396 - val_loss: 0.9271\n",
            "Epoch 159/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5549 - loss: 0.9143 - val_accuracy: 0.5416 - val_loss: 0.9385\n",
            "Epoch 160/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5528 - loss: 0.9106 - val_accuracy: 0.5396 - val_loss: 0.9200\n",
            "Epoch 161/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5541 - loss: 0.9122 - val_accuracy: 0.5390 - val_loss: 0.9187\n",
            "Epoch 162/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5569 - loss: 0.9131 - val_accuracy: 0.5403 - val_loss: 0.9222\n",
            "Epoch 163/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5572 - loss: 0.9134 - val_accuracy: 0.5416 - val_loss: 0.9269\n",
            "Epoch 164/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5536 - loss: 0.9112 - val_accuracy: 0.5386 - val_loss: 0.9472\n",
            "Epoch 165/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5579 - loss: 0.9097 - val_accuracy: 0.5392 - val_loss: 0.9244\n",
            "Epoch 166/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5550 - loss: 0.9077 - val_accuracy: 0.5386 - val_loss: 0.9277\n",
            "Epoch 167/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5564 - loss: 0.9110 - val_accuracy: 0.5402 - val_loss: 0.9216\n",
            "Epoch 168/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5572 - loss: 0.9089 - val_accuracy: 0.5407 - val_loss: 0.9273\n",
            "Epoch 169/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5574 - loss: 0.9119 - val_accuracy: 0.5393 - val_loss: 0.9200\n",
            "Epoch 170/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5568 - loss: 0.9084 - val_accuracy: 0.5404 - val_loss: 0.9291\n",
            "Epoch 171/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5599 - loss: 0.9002 - val_accuracy: 0.5394 - val_loss: 0.9176\n",
            "Epoch 172/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5552 - loss: 0.9096 - val_accuracy: 0.5406 - val_loss: 0.9358\n",
            "Epoch 173/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5570 - loss: 0.9074 - val_accuracy: 0.5386 - val_loss: 0.9495\n",
            "Epoch 174/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5530 - loss: 0.9097 - val_accuracy: 0.5401 - val_loss: 0.9221\n",
            "Epoch 175/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5563 - loss: 0.9089 - val_accuracy: 0.5402 - val_loss: 0.9235\n",
            "Epoch 176/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5580 - loss: 0.9063 - val_accuracy: 0.5401 - val_loss: 0.9279\n",
            "Epoch 177/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5521 - loss: 0.9134 - val_accuracy: 0.5400 - val_loss: 0.9278\n",
            "Epoch 178/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5573 - loss: 0.9167 - val_accuracy: 0.5404 - val_loss: 0.9192\n",
            "Epoch 179/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5611 - loss: 0.9020 - val_accuracy: 0.5391 - val_loss: 0.9196\n",
            "Epoch 180/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5567 - loss: 0.9033 - val_accuracy: 0.5408 - val_loss: 0.9244\n",
            "Epoch 181/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5548 - loss: 0.9095 - val_accuracy: 0.5402 - val_loss: 0.9180\n",
            "Epoch 182/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5547 - loss: 0.9077 - val_accuracy: 0.5404 - val_loss: 0.9168\n",
            "Epoch 183/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5594 - loss: 0.9086 - val_accuracy: 0.5402 - val_loss: 0.9177\n",
            "Epoch 184/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5587 - loss: 0.9079 - val_accuracy: 0.5389 - val_loss: 0.9438\n",
            "Epoch 185/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5602 - loss: 0.9133 - val_accuracy: 0.5395 - val_loss: 0.9180\n",
            "Epoch 186/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5581 - loss: 0.9031 - val_accuracy: 0.5402 - val_loss: 0.9185\n",
            "Epoch 187/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5593 - loss: 0.9091 - val_accuracy: 0.5389 - val_loss: 0.9613\n",
            "Epoch 188/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5524 - loss: 0.9138 - val_accuracy: 0.5403 - val_loss: 0.9174\n",
            "Epoch 189/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5575 - loss: 0.9090 - val_accuracy: 0.5416 - val_loss: 0.9302\n",
            "Epoch 190/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5574 - loss: 0.9053 - val_accuracy: 0.5401 - val_loss: 0.9322\n",
            "Epoch 191/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5529 - loss: 0.9124 - val_accuracy: 0.5394 - val_loss: 0.9196\n",
            "Epoch 192/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5544 - loss: 0.9036 - val_accuracy: 0.5395 - val_loss: 0.9207\n",
            "Epoch 193/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5636 - loss: 0.9031 - val_accuracy: 0.5423 - val_loss: 0.9566\n",
            "Epoch 194/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5571 - loss: 0.9063 - val_accuracy: 0.5390 - val_loss: 0.9215\n",
            "Epoch 195/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5561 - loss: 0.9022 - val_accuracy: 0.5398 - val_loss: 0.9303\n",
            "Epoch 196/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5553 - loss: 0.9081 - val_accuracy: 0.5403 - val_loss: 0.9260\n",
            "Epoch 197/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5568 - loss: 0.9050 - val_accuracy: 0.5389 - val_loss: 0.9201\n",
            "Epoch 198/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5598 - loss: 0.9055 - val_accuracy: 0.5395 - val_loss: 0.9331\n",
            "Epoch 199/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5561 - loss: 0.9114 - val_accuracy: 0.5403 - val_loss: 0.9323\n",
            "Epoch 200/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5536 - loss: 0.9150 - val_accuracy: 0.5392 - val_loss: 0.9218\n",
            "Epoch 201/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5552 - loss: 0.9083 - val_accuracy: 0.5404 - val_loss: 0.9163\n",
            "Epoch 202/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5601 - loss: 0.9149 - val_accuracy: 0.5401 - val_loss: 0.9173\n",
            "Epoch 203/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5571 - loss: 0.9091 - val_accuracy: 0.5404 - val_loss: 0.9202\n",
            "Epoch 204/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5519 - loss: 0.9085 - val_accuracy: 0.5400 - val_loss: 0.9641\n",
            "Epoch 205/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5564 - loss: 0.9098 - val_accuracy: 0.5386 - val_loss: 0.9365\n",
            "Epoch 206/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5604 - loss: 0.9014 - val_accuracy: 0.5404 - val_loss: 0.9175\n",
            "Epoch 207/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5564 - loss: 0.9023 - val_accuracy: 0.5389 - val_loss: 0.9577\n",
            "Epoch 208/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5580 - loss: 0.9102 - val_accuracy: 0.5397 - val_loss: 0.9189\n",
            "Epoch 209/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5573 - loss: 0.9042 - val_accuracy: 0.5401 - val_loss: 0.9164\n",
            "Epoch 210/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5563 - loss: 0.9014 - val_accuracy: 0.5390 - val_loss: 0.9199\n",
            "Epoch 211/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5640 - loss: 0.9065 - val_accuracy: 0.5401 - val_loss: 0.9229\n",
            "Epoch 212/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5565 - loss: 0.9094 - val_accuracy: 0.5402 - val_loss: 0.9183\n",
            "Epoch 213/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5534 - loss: 0.9112 - val_accuracy: 0.5406 - val_loss: 0.9211\n",
            "Epoch 214/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5581 - loss: 0.9090 - val_accuracy: 0.5398 - val_loss: 0.9195\n",
            "Epoch 215/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5593 - loss: 0.9048 - val_accuracy: 0.5404 - val_loss: 0.9212\n",
            "Epoch 216/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5555 - loss: 0.9099 - val_accuracy: 0.5399 - val_loss: 0.9301\n",
            "Epoch 217/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5572 - loss: 0.9080 - val_accuracy: 0.5394 - val_loss: 0.9327\n",
            "Epoch 218/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5569 - loss: 0.9027 - val_accuracy: 0.5412 - val_loss: 0.9176\n",
            "Epoch 219/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5553 - loss: 0.9132 - val_accuracy: 0.5387 - val_loss: 0.9678\n",
            "Epoch 220/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5540 - loss: 0.9081 - val_accuracy: 0.5383 - val_loss: 0.9209\n",
            "Epoch 221/300\n",
            "\u001b[1m351/351\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5524 - loss: 0.9104 - val_accuracy: 0.5389 - val_loss: 0.9237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set file paths\n",
        "zip_file_path = '/content/drive/My Drive/SP500_data2020-2024.zip'  # Replace with the path to the file in Google Drive\n",
        "output_dir = '/content/SP500_data2020-2024'  # Extract to Colab working directory\n",
        "\n",
        "# Extract files\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_dir)\n",
        "\n",
        "print(\"Files extracted to:\", output_dir)\n",
        "\n",
        "# List all CSV files in the extracted directory\n",
        "csv_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.csv')])\n",
        "print(f\"Found {len(csv_files)} CSV files.\")\n",
        "print(\"Sample files:\", csv_files[:5])  # Print the first 5 files alphabetically\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read all CSV files into a dictionary\n",
        "sp500_data = {}\n",
        "for csv_file in csv_files:\n",
        "    symbol = csv_file.replace('.csv', '')  # Extract the stock symbol\n",
        "    file_path = os.path.join(output_dir, csv_file)\n",
        "    sp500_data[symbol] = pd.read_csv(file_path)\n",
        "\n",
        "# Filter symbols to keep only those with the most common data length\n",
        "def filter_symbols_by_most_common_length(sp500_data):\n",
        "    # Calculate the length of data for each symbol\n",
        "    lengths = {symbol: len(data) for symbol, data in sp500_data.items()}\n",
        "\n",
        "    # Count the frequency of each length\n",
        "    length_counts = pd.Series(lengths).value_counts()\n",
        "\n",
        "    # Find the most common length\n",
        "    most_common_length = length_counts.idxmax()\n",
        "    print(f\"Most common length: {most_common_length}, Count: {length_counts.max()}\")\n",
        "\n",
        "    # Filter symbols with the most common length\n",
        "    filtered_symbols = [symbol for symbol, length in lengths.items() if length == most_common_length]\n",
        "\n",
        "    # Return filtered data and the most common length\n",
        "    filtered_data = {symbol: sp500_data[symbol] for symbol in filtered_symbols}\n",
        "    return filtered_data, most_common_length\n",
        "\n",
        "# Filter the data\n",
        "filtered_sp500_data, most_common_length = filter_symbols_by_most_common_length(sp500_data)\n",
        "\n",
        "# Check the filtered result\n",
        "print(f\"Filtered data count: {len(filtered_sp500_data)}\")\n",
        "\n",
        "# Window length\n",
        "window_size = 25\n",
        "degree = 2\n",
        "\n",
        "# Data collection\n",
        "data_set = {'matrices_4x4': [], 'labels1': [], 'labels2': [], 'labels3': []}\n",
        "\n",
        "# Create matrices and labels for each stock\n",
        "for index_name, data in filtered_sp500_data.items():\n",
        "    # Ensure 'Open' and 'Close' columns are numeric\n",
        "    data['Open'] = pd.to_numeric(data['Open'], errors='coerce')\n",
        "    data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
        "\n",
        "    # Extract 'Open' and 'Close' values, skipping the first two rows\n",
        "    open_values = data['Open'].values[2:]\n",
        "    close_values = data['Close'].values[2:]\n",
        "\n",
        "    # Build window matrices\n",
        "    num_samples = len(data) - window_size + 1\n",
        "    for start in range(num_samples - 2):  # Iterate through all data points\n",
        "        # Extract window data\n",
        "        open_row = open_values[start:start + window_size]\n",
        "        close_row = close_values[start:start + window_size]\n",
        "\n",
        "        # Normalize using the 16th last value of close_row\n",
        "        normalization_factor = close_row[-16]\n",
        "        open_row = open_row / normalization_factor\n",
        "        close_row = close_row / normalization_factor\n",
        "\n",
        "        # Create an alternating combined array\n",
        "        combined = np.array([open_row[i // 2] if i % 2 == 0 else close_row[i // 2] for i in range(window_size * 2)])\n",
        "        matrix_4x4 = combined[:-30].reshape(4, 5, 1)\n",
        "        result = [combined[-32], combined[-31], combined[-1]]\n",
        "\n",
        "        # Perform higher-order fitting on combined data\n",
        "        x = np.arange(len(combined[-4:-1]))\n",
        "        coeffs = poly.polyfit(x, result, deg=degree)\n",
        "\n",
        "        # Calculate first and second derivatives\n",
        "        first_derivative_coeffs = poly.polyder(coeffs)\n",
        "        first_derivatives = poly.polyval(x, first_derivative_coeffs)\n",
        "        second_derivative_coeffs = poly.polyder(first_derivative_coeffs)\n",
        "        second_derivatives = poly.polyval(x, second_derivative_coeffs)\n",
        "\n",
        "        # Calculate labels\n",
        "        label1 = 1 if first_derivatives[-1] > 0 else 0\n",
        "        label2 = 1 if second_derivatives[-1] > 0 else 0\n",
        "        label3 = 1 if close_row[-1] > close_row[-16] else 0\n",
        "\n",
        "        # Add to the unified data collection\n",
        "        data_set['matrices_4x4'].append(matrix_4x4)\n",
        "        data_set['labels1'].append(label1)\n",
        "        data_set['labels2'].append(label2)\n",
        "        data_set['labels3'].append(label3)\n",
        "\n",
        "# Convert data to NumPy arrays\n",
        "matrices_4x4 = np.array(data_set['matrices_4x4'])\n",
        "labels1 = np.array(data_set['labels1'])\n",
        "labels2 = np.array(data_set['labels2'])\n",
        "labels3 = np.array(data_set['labels3'])\n",
        "\n",
        "# Output results for verification\n",
        "print(f\"4x4 matrices shape: {matrices_4x4.shape}\")\n",
        "print(f\"Labels1 shape: {labels1.shape}\")\n",
        "print(f\"Labels2 shape: {labels2.shape}\")\n",
        "print(f\"Labels3 shape: {labels3.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K7d1ti7-x-M",
        "outputId": "52979672-ce9b-4234-a4be-04adedde436c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Files extracted to: /content/SP500_data2020-2024\n",
            "Found 501 CSV files.\n",
            "Sample files: ['A.csv', 'AAPL.csv', 'ABBV.csv', 'ABNB.csv', 'ABT.csv']\n",
            "Most common length: 1239, Count: 489\n",
            "Filtered data count: 489\n",
            "4x4 matrices shape: (593157, 4, 5, 1)\n",
            "Labels1 shape: (593157,)\n",
            "Labels2 shape: (593157,)\n",
            "Labels3 shape: (593157,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "\n",
        "# Get model prediction probabilities\n",
        "predictions2_pro = model1.predict(matrices_4x4)\n",
        "\n",
        "# Get predicted labels\n",
        "predicted_classes = np.argmax(predictions2_pro, axis=1)\n",
        "\n",
        "# Get the total number of classes\n",
        "num_classes = predictions2_pro.shape[1]\n",
        "\n",
        "# Initialize the final prediction array\n",
        "final_predictions = np.full(predictions2_pro.shape[0], -1)  # Initialize with -1 to indicate unclassified\n",
        "\n",
        "# Define the threshold range\n",
        "thresholds = np.arange(0.5, 0.95, 0.05)  # From 0.5 to 0.95 with a step of 0.05\n",
        "\n",
        "# Iterate over each threshold for evaluation\n",
        "for threshold in thresholds:\n",
        "    # Reset the prediction array\n",
        "    final_predictions[:] = -1\n",
        "\n",
        "    # Iterate over each class label to process samples predicted as that class\n",
        "    for class_label in range(num_classes):\n",
        "        # Find indices of samples predicted as the current class\n",
        "        class_indices = np.where(predicted_classes == class_label)[0]\n",
        "\n",
        "        if len(class_indices) > 0:\n",
        "            # Get probabilities for these samples\n",
        "            class_probs = predictions2_pro[class_indices, class_label]\n",
        "\n",
        "            # Filter indices where probability exceeds the threshold\n",
        "            selected_indices = class_indices[class_probs >= threshold]\n",
        "\n",
        "            # Mark as odd (1) or even (0)\n",
        "            odd_or_even = 1 if class_label % 2 == 1 else 0\n",
        "            final_predictions[selected_indices] = odd_or_even\n",
        "\n",
        "    # Filter out valid predictions\n",
        "    valid_indices = final_predictions != -1\n",
        "    filtered_final_predictions = final_predictions[valid_indices]\n",
        "    filtered_true_labels = labels3[valid_indices]  # Corresponding true labels\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(filtered_final_predictions == filtered_true_labels)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(filtered_true_labels, filtered_final_predictions, labels=[0, 1])\n",
        "\n",
        "    # Calculate Precision\n",
        "    tp = cm[1, 1]  # True Positives\n",
        "    fp = cm[0, 1]  # False Positives\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0  # Handle division by zero\n",
        "\n",
        "    # Calculate Recall\n",
        "    fn = cm[1, 0]  # False Negatives\n",
        "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0  # Handle division by zero\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Threshold: {threshold:.2f}\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "    print(f\"Precision (TP / (TP + FP)): {precision:.2%}\")\n",
        "    print(f\"Recall (TP / (TP + FN)): {recall:.2%}\")\n",
        "    print(f\"F1 Score: {f1:.2%}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t1sgBz1B1cR",
        "outputId": "535e9068-b576-4715-fc22-8e1b75607d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18537/18537\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step\n",
            "Threshold: 0.50\n",
            "Accuracy: 55.18%\n",
            "Precision (TP / (TP + FP)): 55.26%\n",
            "Recall (TP / (TP + FN)): 99.44%\n",
            "F1 Score: 71.04%\n",
            "Confusion Matrix:\n",
            "[[   957 215846]\n",
            " [  1503 266636]]\n",
            "\n",
            "Threshold: 0.55\n",
            "Accuracy: 55.29%\n",
            "Precision (TP / (TP + FP)): 55.33%\n",
            "Recall (TP / (TP + FN)): 99.75%\n",
            "F1 Score: 71.18%\n",
            "Confusion Matrix:\n",
            "[[   287 159375]\n",
            " [   488 197372]]\n",
            "\n",
            "Threshold: 0.60\n",
            "Accuracy: 62.39%\n",
            "Precision (TP / (TP + FP)): 63.46%\n",
            "Recall (TP / (TP + FN)): 95.94%\n",
            "F1 Score: 76.39%\n",
            "Confusion Matrix:\n",
            "[[  86 1959]\n",
            " [ 144 3402]]\n",
            "\n",
            "Threshold: 0.65\n",
            "Accuracy: 70.91%\n",
            "Precision (TP / (TP + FP)): 71.19%\n",
            "Recall (TP / (TP + FN)): 98.79%\n",
            "F1 Score: 82.75%\n",
            "Confusion Matrix:\n",
            "[[  31  758]\n",
            " [  23 1873]]\n",
            "\n",
            "Threshold: 0.70\n",
            "Accuracy: 73.55%\n",
            "Precision (TP / (TP + FP)): 73.54%\n",
            "Recall (TP / (TP + FN)): 99.87%\n",
            "F1 Score: 84.71%\n",
            "Confusion Matrix:\n",
            "[[   6  549]\n",
            " [   2 1526]]\n",
            "\n",
            "Threshold: 0.75\n",
            "Accuracy: 75.39%\n",
            "Precision (TP / (TP + FP)): 75.36%\n",
            "Recall (TP / (TP + FN)): 100.00%\n",
            "F1 Score: 85.95%\n",
            "Confusion Matrix:\n",
            "[[   2  406]\n",
            " [   0 1242]]\n",
            "\n",
            "Threshold: 0.80\n",
            "Accuracy: 77.72%\n",
            "Precision (TP / (TP + FP)): 77.72%\n",
            "Recall (TP / (TP + FN)): 100.00%\n",
            "F1 Score: 87.46%\n",
            "Confusion Matrix:\n",
            "[[  0 285]\n",
            " [  0 994]]\n",
            "\n",
            "Threshold: 0.85\n",
            "Accuracy: 78.15%\n",
            "Precision (TP / (TP + FP)): 78.15%\n",
            "Recall (TP / (TP + FN)): 100.00%\n",
            "F1 Score: 87.74%\n",
            "Confusion Matrix:\n",
            "[[  0 208]\n",
            " [  0 744]]\n",
            "\n",
            "Threshold: 0.90\n",
            "Accuracy: 77.95%\n",
            "Precision (TP / (TP + FP)): 77.95%\n",
            "Recall (TP / (TP + FN)): 100.00%\n",
            "F1 Score: 87.61%\n",
            "Confusion Matrix:\n",
            "[[  0 142]\n",
            " [  0 502]]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}